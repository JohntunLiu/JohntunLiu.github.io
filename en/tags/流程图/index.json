[{"content":"使用element ui 的:tree-props展示树形结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  \u0026lt;!-- 列表 --\u0026gt; \u0026lt;el-table v-loading=\u0026#34;loading\u0026#34; :data=\u0026#34;list\u0026#34; row-key=\u0026#34;appOrderNo\u0026#34; default-expand-all :tree-props=\u0026#34;{children: \u0026#39;children\u0026#39;}\u0026#34; \u0026gt; \u0026lt;el-table-column type=\u0026#34;index\u0026#34; label=\u0026#34;序号\u0026#34; width=\u0026#34;55\u0026#34; prop=\u0026#34;parentIndex\u0026#34;\u0026gt; \u0026lt;template slot-scope=\u0026#34;scope\u0026#34;\u0026gt; \u0026lt;span v-if=\u0026#34;scope.row.isIndex \u0026#34;\u0026gt;{{scope.row.parentIndex}}\u0026lt;/span\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;/el-table-column\u0026gt; \u0026lt;el-table-column label=\u0026#34;订单号\u0026#34; align=\u0026#34;center\u0026#34; prop=\u0026#34;appOrderNo\u0026#34; /\u0026gt; \u0026lt;el-table-column label=\u0026#34;交易时间\u0026#34; align=\u0026#34;center\u0026#34; prop=\u0026#34;createTime\u0026#34; width=\u0026#34;180\u0026#34;\u0026gt; \u0026lt;template slot-scope=\u0026#34;scope\u0026#34;\u0026gt; \u0026lt;span\u0026gt;{{ parseTime(scope.row.createTime) }}\u0026lt;/span\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;/el-table-column\u0026gt; .... \u0026lt;/el-table\u0026gt;   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  /** 查询列表 */ getList() { this.loading = true; // 处理查询参数  let params = {...this.queryParams}; this.addBeginAndEndTime(params, this.dateRangeCreateTime, \u0026#39;createTime\u0026#39;); // 执行查询  getPayOrderPage(params).then(response =\u0026gt; { this.list = response.data.list; //计算序号，标注父标签  this.list.forEach((item, index) =\u0026gt; { item.isIndex = true; const {pageSize, pageNo} = this.queryParams; // rows列数 page 页码  item.parentIndex = index + pageSize * (pageNo - 1) + 1 // item.parentIndex = index + 1;  }); this.total = response.data.total; this.loading = false; }); }   ","description":"","id":2,"section":"posts","tags":["vue"],"title":"vue处理树型显示表格序号问题","uri":"/en/posts/vue/"},{"content":"Operating Systems: Virtualization, Concurrency \u0026amp; Persistence Introduction So, what happens when a program runs? Well, a running program does one very simple thing: it executes instructions. Many millions (and these days, even billions) of times every second, the processor fetches an instruction from memory, decodes it (i.e., figures out which instruction this is), and executes it (i.e., it does the thing that it is supposed to do, like add two numbers together, access memory, check a condition, jump to a function, and so forth). After it is done with this instruction, the processor moves on to the next instruction, and so on, and so on, until the program finally completes.\nVirtualization The CPU The primary way the OS does this is through a general technique that we call virtualization. That is, the OS takes a physical resource (such as the processor, or memory, or a disk) and transforms it into a more general, powerful, and easy-to-use virtual form of itself. Thus, we sometimes refer to the operating system as a virtual machine.\nA typical OS, in fact, exports a few hundred system calls that are available to applications. Because the OS provides these calls to run programs, access memory and devices, and other related actions, we also sometimes say that the OS provides a standard library to applications.\nVirtualizing Memory Memory is just an array of bytes; to read memory, one must specify an address to be able to access the data stored there; to write (or update) memory, one must also specify the data to be written to the given address.\nEach process accesses its own private virtual address space (sometimes just called its address space), which the OS somehow maps onto the physical memory of the machine.\nConcurrency When there are many concurrently executing threads within the same memory space, how can we build a correctly working program? What primitives are needed from the OS? What mechanisms should be provided by the hardware? How can we use them to solve the problems of concurrency?\nPersistence We need hardware and software to be able to store data persistently; such storage is thus critical to any system as users care a great deal about their data.\nThe hardware comes in the form of some kind of input/output or I/O device; in modern systems, a hard drive is a common repository for long-lived information, although solid-state drives (SSDs) are making headway in this arena as well.\nThe software in the operating system that usually manages the disk is called the file system; it is thus responsible for storing any files the user creates in a reliable and efficient manner on the disks of the system.\nDesign Goals   Requirements\nSo, now you have some idea of what an OS actually does: it takes physical resources, such as a CPU, memory, or disk, and virtualizes them. It handles tough and tricky issues related to concurrency. And it stores files persistently, thus making them safe over the long-term.\n  Abstractions\nAbstraction makes it possible to write a large program by dividing it into small and understandable pieces, to write such a program in a high-level language like C without thinking about assembly, to write code in assembly without thinking about logic gates, and to build a processor out of gates without thinking too much about transistors.\n  High performance: minimum cost\nVirtualization and making the system easy to use are well worth it, but not at any cost; thus, we must strive to provide virtualization and other OS features without excessive overheads.\nThese overheads arise in a number of forms: extra time (more instructions) and extra space (in memory or on disk).\n  Protection\nAnother goal will be to provide protection between applications, as well as between the OS and applications. Because we want to make sure that the malicious or accidental bad behavior of one does not harm others or OS.\nProtection is at the heart of one of the main principles underlying an operating system, which is that of isolation; isolating processes from one another is the key to protection and thus underlies much of what an OS must do.\n  Reliability\nThe operating system must also run non-stop; when it fails, all applications running on the system fail as well.\n  Other goals\nenergy-efficiency is important in our increasingly green world;\nsecurity (an extension of protection, really) against malicious applications is critical, especially in these highly-networked times;\nmobility is increasingly important as OSes are run on smaller and smaller devices.\n  Virtualization: Processes The definition of a process, informally, is quite simple: it is a running program.\nTime sharing The OS creates this illusion by virtualizing the CPU. By running one process, then stopping it and running another, and so forth, the OS can promote the illusion that many virtual CPUs exist when in fact there is only one physical CPU (or a few).\nThe counterpart of time sharing is space sharing, where a resource is divided (in space) among those who wish to use it. For example, disk space is naturally a space-shared resource; once a block is assigned to a file, it is normally not assigned to another file until the user deletes the original file.\nPolicies Policies are algorithms for making some kind of decision within the OS. For example, given a number of possible programs to run on a CPU, which program should the OS run? A scheduling policy in the OS will make this decision, likely using historical information (e.g., which program has run more over the last minute?)\nIn many operating systems, a common design paradigm is to separate high-level policies from their low-level mechanisms.\nThe Abstraction: A Process ​\tThe abstraction provided by the OS of a running program is something we will call a process.\n  How memory constitutes a process?\nOne obvious component of a machine state that comprises a process is its memory. Instructions lie in memory; the data that the running program reads and writes sits in memory as well. Thus the memory that the process can address (called its address space) is part of the process.\n  Registers\nAlso part of the process’s machine state are registers; many instructions explicitly read or update registers and thus clearly they are important to the execution of the process.\nNote that there are some particularly special registers that form part of this machine state. For example, the program counter (PC) (sometimes called the instruction pointer or IP) tells us which instruction of the program will execute next; similarly a stack pointer and associated frame pointer are used to manage the stack for function parameters, local variables, and return addresses.\n  Process API   Create\nAn operating system must include some method to create new processes. When you type a command into the shell, or double-click on an application icon, the OS is invoked to create a new process to run the program you have indicated.\n  Destroy\nAs there is an interface for process creation, systems also provide an interface to destroy processes forcefully. Of course, many processes will run and just exit by themselves when complete; when they don’t, however, the user may wish to kill them, and thus an interface to halt a runaway process is quite useful.\n  Wait\nSometimes it is useful to wait for a process to stop running; thus some kind of waiting interface is often provided.\n  Miscellaneous control\nFor example, most operating systems provide some kind of method to suspend a process (stop it from running for a while) and then resume it (continue it running).\n  Status\nThere are usually interfaces to get some status information about a process as well, such as how long it has run for, or what state it is in.\n  Process Creation: A Little More Detail Specifically, how does the OS get a program up and running? How does process creation actually work?\n  Transforming a program into a process\nThe first thing that the OS must do to run a program is to load its code and any static data (e.g., initialized variables) into memory, into the address space of the process.\n  Memory allocation\nSome memory must be allocated for the program’s run-time stack (or just stack). As you should likely already know, C programs use the stack for local variables, function parameters, and return addresses; the OS allocates this memory and gives it to the process.\nThe OS may also allocate some memory for the program’s heap. In C programs, the heap is used for explicitly requested dynamically-allocated data; programs request such space by calling malloc() and free it explicitly by calling free().The heap is needed for data structures such as linked lists, hash tables, trees, and other interesting data structures. The heap will be small at first; as the program runs, and requests more memory via the malloc() library API, the OS may get involved and allocate more memory to the process to help satisfy such calls.\n  I/O related setup\nBy loading the code and static data into memory, creating and initializing a stack, and doing other work as related to I/O setup, the OS has now (finally) set the stage for program execution.\n  Processes States   The three states of a process\nIn a simplified view, a process can be in one of three states:\nRunning: In the running state, a process is running on a processor. This means it is executing instructions.\nReady: In the ready state, a process is ready to run but for some reason, the OS has chosen not to run it at this given moment.\nBlocked: In the blocked state, a process has performed some kind of operation that makes it not ready to run until some other event takes place. A common example: when a process initiates an I/O request to a disk, it becomes blocked and thus some other process can use the processor.\n  Transitioning from one state to another\n  Data Structures   Information of a process that the OS tracks\nThe snippet below shows what type of information an OS needs to track about each process in the xv6 kernel.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  // the registers xv6 will save and restore // to stop and subsequently restart a process struct context { int eip; int esp; int ebx; int ecx; int edx; int esi; int edi; int ebp; }; // the different states a process can be in enum proc_state { UNUSED, EMBRYO, SLEEPING, RUNNABLE, RUNNING, ZOMBIE }; // the information xv6 tracks about each process // including its register context and state struct proc { char *mem; // Start of process memory  uint sz; // Size of process memory  char *kstack; // Bottom of kernel stack  // for this process  enum proc_state state; // Process state  int pid; // Process ID  struct proc *parent; // Parent process  void *chan; // If !zero, sleeping on chan  int killed; // If !zero, has been killed  struct file *ofile[NOFILE]; // Open files  struct inode *cwd; // Current directory  struct context context; // Switch here to run process  struct trapframe *tf; // Trap frame for the  // current interrupt };   Summary the low-level mechanisms needed to implement processes, and the higher-level policies required to schedule them in an intelligent way.\n ASIDE: KEY PROCESS TERMS\n The process is the major OS abstraction of a running program. At any point in time, the process can be described by its state: the contents of memory in its address space, the contents of CPU registers (including the program counter and stack pointer, among others), and information about I/O (such as open files which can be read or written). The process API consists of calls that programs can make related to processes. Typically, this includes creation, destruction, and other useful calls. Processes exist in one of many different process states, including running, ready to run, and blocked. Different events (e.g., getting scheduled or descheduled, or waiting for an I/O to complete) transition a process from one of these states to the other. A process list contains information about all processes in the system. Each entry is found in what is sometimes called a process control block (PCB), which is really just a structure that contains information about a specific process.     Virtualization: Process API What interfaces should the OS present for process creation and control? How should these interfaces be designed to enable powerful functionality, ease of use, and high performance?\nThe fork() System Call The fork() system call is used to create a new process.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; int main(int argc, char* argv[]){ printf(\u0026#34;hello world (pid:%d)\\n\u0026#34;, (int)getpid()); fflush(stdout); int rc = fork(); int i = 0; if (rc \u0026lt; 0) { // fork failed  fprintf(stderr, \u0026#34;fork failed\\n\u0026#34;); exit(1); } else if (rc == 0){ // child (new process)  printf(\u0026#34;hello, I am child (pid:%d)\\n\u0026#34;, (int)getpid()); } else{ // parent goes down this path (main)  for(i;i\u0026lt;999999;i++){} printf(\u0026#34;hello, I am parents of %d (pid:%d)\\n\u0026#34;, rc, (int)getpid()); } return 0; } //Output // hello world (pid:15) // hello, I am child (pid:16) // hello, I am parents of 16 (pid:15)   The process calls the fork() system call, which the OS provides as a way to create a new process. The odd part: the process that is created is an (almost) exact copy of the calling process. That means that to the OS, it now looks like there are two copies of the program p1 running, and both are about to return from the fork() system call. The newly-created process (called the child, in contrast to the creating parent) doesn’t start running at main() like you might expect (note, the “hello, world” message only got printed out once); rather, it just comes into life as if it had called fork() itself.\nSpecifically, while the parent receives the PID of the newly-created child, the child receives a return code of zero. This differentiation is useful because it is simple then to write the code that handles the two different cases (as above).\nNon-deterministic output The CPU scheduler, a topic we’ll discuss in great detail soon, determines which process runs at a given moment in time; because the scheduler is complex, we cannot usually make strong assumptions about what it will choose to do, and hence which process will run first. This non-determinism, as it turns out, leads to some interesting problems, particularly in multi-threaded programs.\nThe wait() System Call It is quite useful for a parent to wait for a child process to finish what it has been doing.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;sys/wait.h\u0026gt; int main(int argc, char* argv[]) { printf(\u0026#34;hello world (pid:%d)\\n\u0026#34;, (int)getpid()); fflush(stdout); int rc = fork(); if (rc \u0026lt; 0) { // fork failed; exit  fprintf(stderr, \u0026#34;fork failed\\n\u0026#34;); exit(1); } else if (rc == 0) { // child (new process)  printf(\u0026#34;hello, I am child (pid:%d)\\n\u0026#34;, (int)getpid()); } else { // parent goes down this path (main)  int rc_wait = wait(NULL); printf(\u0026#34;hello, I am parent of %d (rc_wait:%d) (pid:%d)\\n\u0026#34;, rc, rc_wait, (int)getpid()); } return 0; } // output: // hello world (pid:15) // hello, I am child (pid:16) // hello, I am parent of 16 (rc_wait:16) (pid:15)   Thus, even when the parent runs first, it politely waits for the child to finish running, then wait() returns, and then the parent prints its message.\nFinally, The exec() System Call To run a different program; exec() does just that (see the code below).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;sys/wait.h\u0026gt; int main(int argc, char *argv[]){ printf(\u0026#34;hello world (pid:%d)\\n\u0026#34;, (int) getpid()); fflush(stdout); int rc = fork(); if (rc \u0026lt; 0) { // fork failed; exit  fprintf(stderr, \u0026#34;fork failed\\n\u0026#34;); exit(1); } else if (rc == 0) { // child (new process)  printf(\u0026#34;hello, I am child (pid:%d)\\n\u0026#34;, (int) getpid()); fflush(stdout); char *myargs[3]; myargs[0] = strdup(\u0026#34;wc\u0026#34;); // program: \u0026#34;wc\u0026#34; (word count)  myargs[1] = strdup(\u0026#34;p3.c\u0026#34;); // argument: file to count  myargs[2] = NULL; // marks end of array  execvp(myargs[0], myargs); // runs word count  printf(\u0026#34;this shouldn\u0026#39;t print out\u0026#34;); } else { // parent goes down this path (original process)  int wc = wait(NULL); printf(\u0026#34;hello, I am parent of %d (wc:%d) (pid:%d)\\n\u0026#34;, rc, wc, (int) getpid()); } return 0; } // output // hello world (pid:14) // hello, I am child (pid:15) // 31 124 1008 p3.c // hello, I am parent of 15 (wc:15) (pid:14)   In this example, the child process calls execvp() in order to run the program wc, which is the word counting program. In fact, it runs wc on the source file p3.c, thus telling us how many lines, words, and bytes are found in the file.\nWhat it does: given the name of an executable (e.g., wc), and some arguments (e.g., p3.c), it loads code (and static data) from that executable and overwrites its current code segment (and current static data) with it; the heap and stack and other parts of the memory space of the program are re-initialized. Then the OS simply runs that program, passing in any arguments as the argv of that process. Thus, it does not create a new process; rather, it transforms the currently running program (formerly p3) into a different running program (wc). After the exec() in the child, it is almost as if p3.c never ran; a successful call to exec() never returns.\nWhy? Motivating the API Shell The shell is just a user program.\nIt shows you a prompt and then waits for you to type something into it. You then type a command into it; in most cases, the shell then figures out where in the file system the executable resides, calls fork() to create a new child process to run the command, calls some variant of exec() to run the command, and then waits for the command to complete by calling wait(). When the child completes, the shell returns from wait() and prints out a prompt again, ready for your next command.\nThe separation of fork() and exec() allows the shell to do a whole bunch of useful things rather easily.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt;#include \u0026lt;assert.h\u0026gt;#include \u0026lt;sys/wait.h\u0026gt; int main(int argc, char *argv[]) { int rc = fork(); if (rc \u0026lt; 0) { // fork failed; exit  fprintf(stderr, \u0026#34;fork failed\\n\u0026#34;); exit(1); } else if (rc == 0) { // child: redirect standard output to a file \tclose(STDOUT_FILENO); open(\u0026#34;./output/p4.output\u0026#34;, O_CREAT|O_WRONLY|O_TRUNC, S_IRWXU); // now exec \u0026#34;wc\u0026#34;...  char *myargs[3]; myargs[0] = strdup(\u0026#34;wc\u0026#34;); // program: \u0026#34;wc\u0026#34; (word count)  myargs[1] = strdup(\u0026#34;p4.c\u0026#34;); // argument: file to count  myargs[2] = NULL; // marks end of array  execvp(myargs[0], myargs); // runs word count  } else { // parent goes down this path (original process)  int wc = wait(NULL); assert(wc \u0026gt;= 0); } return 0; } // prompt\u0026gt; ./p4 // prompt\u0026gt; cat p4.output // 32 109 846 p4.c // prompt\u0026gt;   The program p4 did indeed call fork() to create a new child, and then run the wc program via a call to execvp(). You don’t see any output printed to the screen because it has been redirected to the file p4.output (present in the output directory). Second, you can see that when we view the output file, all the expected output from running wc is found. Cool, right?\nPipeline UNIX pipes are implemented in a similar way but with the pipe() system call. In this case, the output of one process is connected to an in-kernel pipe (i.e., queue), and the input of another process is connected to that same pipe; thus, the output of one process seamlessly is used as input to the next, and long and useful chains of commands can be strung together.\nProcess Control and Users Sending signals to a process Beyond fork(), exec(), and wait(), there are a lot of other interfaces for interacting with processes in UNIX systems. For example, the kill() system call is used to send signals to a process, including directives to pause, die, and other useful imperatives.\nThe entire signals subsystem provides a rich infrastructure to deliver external events to processes, including ways to receive and process those signals within individual processes, and ways to send signals to individual processes as well as entire process groups. To use this form of communication, a process should use the signal() system call to “catch” various signals; doing so ensures that when a particular signal is delivered to a process, it will suspend its normal execution and run a particular piece of code in response to the signal.\nGiving control of processes to users This naturally raises the question: who can send a signal to a process, and who cannot? Generally, the systems we use can have multiple people using them at the same time; if one of these people can arbitrarily send signals such as SIGINT (to interrupt a process, likely terminating it), the usability and security of the system will be compromised. As a result, modern systems include a strong conception of the notion of a user. The user, after entering a password to establish credentials, logs in to gain access to system resources. Users generally can only control their own processes; it is the job of the operating system to parcel out resources (such as CPU, memory, and disk) to each user (and their processes) to meet overall system goals.\nVirtualization: Direct Execution By time sharing the CPU in this manner, virtualization is achieved. There are a few challenges, however, in building such virtualization machinery. The first is performance: how can we implement virtualization without adding excessive overhead to the system? The second is control: how can we run processes efficiently while retaining control over the CPU? Control is particularly important to the OS, as it is in charge of resources; without control, a process could simply run forever and take over the machine, or access information that it should not be allowed to access. Obtaining high performance while maintaining control is thus one of the central challenges in building an operating system.\nBasic Technique: Limited Direct Execution To make a program run as fast as one might expect, not surprisingly OS developers came up with a technique, which we call limited direct execution.\nThe “direct execution” part of the idea is simple: just run the program directly on the CPU. Thus, when the OS wishes to start a program running, it creates a process entry for it in a process list, allocates some memory for it, loads the program code into memory (from disk), locates its entry point (i.e., the main() routine or something similar), jumps to it, and starts running the user’s code. The figure below shows this basic direct execution protocol (without any limits, yet), using a normal call and return to jump to the program’s main() and later back into the kernel.\nThis approach gives rise to a few problems in our quest to virtualize the CPU.\n The first : if we just run a program, how can the OS make sure the program doesn’t do anything that we don’t want it to do, while still running it efficiently? The second: when we are running a process, how does the operating system stop it from running and switch to another process, thus implementing the time sharing we require to virtualize the CPU?  Problem #1: Restricted Operations Process modes user mode; code that runs in user mode is restricted in what it can do. For example, when running in user mode, a process can’t issue I/O requests; doing so would result in the processor raising an exception; the OS would then likely kill the process.\nkernel mode, which the operating system (or kernel) runs in. In this mode, code that runs can do what it likes, including privileged operations such as issuing I/O requests and executing all types of restricted instructions.\nExecuting system calls What should a user process do when it wishes to perform some kind of privileged operation, such as reading from disk? To enable this, virtually all modern hardware provides the ability for user programs to perform a system call.\nSpecial trap instructions To execute a system call, a program must execute a special trap instruction. This instruction simultaneously jumps into the kernel and raises the privilege level to kernel mode; once in the kernel, the system can now perform whatever privileged operations are needed (if allowed) and thus do the required work for the calling process. When finished, the OS calls a special return-from-trap instruction, which, as you might expect, returns into the calling user program while simultaneously reducing the privilege level back to user mode.\nThere is one important detail left out of this discussion: how does the trap know which code to run inside the OS? Clearly, the calling process can’t specify an address to jump to (as you would when making a procedure call); doing so would allow programs to jump anywhere into the kernel which clearly is a Very Bad Idea. Thus the kernel must carefully control what code executes upon a trap.\nThe kernel does so by setting up a trap table at boot time. When the machine boots up, it does so in privileged (kernel) mode and thus is free to configure machine hardware as need be. One of the first things the OS thus does is to tell the hardware what code to run when certain exceptional events occur.\nThe OS informs the hardware of the locations of these trap handlers, usually with some kind of special instruction. Once the hardware is informed, it remembers the location of these handlers until the machine is next rebooted, and thus the hardware knows what to do (i.e., what code to jump to) when system calls and other exceptional events take place.\n In general, a secure system must treat user inputs with great suspicion.\n To specify the exact system call, a system-call number is usually assigned to each system call. The user code is thus responsible for placing the desired system-call number in a register or at a specified location on the stack; the OS, when handling the system call inside the trap handler, examines this number, ensures it is valid, and, if it is, executes the corresponding code. This level of indirection serves as a form of protection; user code cannot specify an exact address to jump to, but rather must request a particular service via number.\nOne last aside: being able to execute the instruction to tell the hardware where the trap tables are is a very powerful capability. Thus, as you might have guessed, it is also a privileged operation.\nThe timeline (with time increasing downward, in the figure below) summarizes the protocol. We assume each process has a kernel stack where registers (including general-purpose registers and the program counter) are saved to and restored from (by the hardware) when transitioning into and out of the kernel.\nThere are two phases in the limited direct execution (LDE) protocol. In the first (at boot time), the kernel initializes the trap table, and the CPU remembers its location for subsequent use. The kernel does so via a privileged instruction (all privileged instructions are highlighted in bold).\nIn the second (when running a process), the kernel sets up a few things (e.g., allocating a node on the process list, allocating memory) before using a return-from-trap instruction to start the execution of the process; this switches the CPU to user mode and begins running the process. When the process wishes to issue a system call, it traps back into the OS, which handles it and once again returns control via a return-from-trap to the process. The process then completes its work, and returns from main(); this usually will return into some stub code which will properly exit the program (say, by calling the exit() system call, which traps into the OS). At this point, the OS cleans up and we are done.\nProblem #2: Switching Between Processes How can the operating system regain control of the CPU so that it can switch between processes?\nA cooperative approach: wait for system calls In this style, the OS trusts the processes of the system to behave reasonably. Processes that run for too long are assumed to periodically give up the CPU so that the OS can decide to run some other task.\nIn a cooperative scheduling system, the OS regains control of the CPU by waiting for a system call or an illegal operation of some kind to take place.\nA non-cooperative approach: the OS takes control A timer device can be programmed to raise an interrupt every so many milliseconds; when the interrupt is raised, the currently running process is halted, and a pre-configured interrupt handler in the OS runs.\nAs we discussed before with system calls, the OS must inform the hardware of which code to run when the timer interrupt occurs; thus, at boot time, the OS does exactly that. Second, also during the boot sequence, the OS must start the timer, which is, of course, a privileged operation. Once the timer has begun, the OS can thus feel safe in that control will eventually be returned to it, and thus the OS is free to run user programs. The timer can also be turned off (also a privileged operation)\nSaving and restoring context Whether to continue running the currently-running process or switch to a different one.\nIf the decision is made to switch, the OS then executes a low-level piece of code which we refer to as a context switch. A context switch is conceptually simple: all the OS has to do is save a few register values for the currently-executing process (onto its kernel stack, for example) and restore a few for the soon-to-be-executing process (from its kernel stack). By doing so, the OS thus ensures that when the return-from-trap instruction is finally executed, instead of returning to the process that was running, the system resumes execution of another process.\nTo save the context of the currently-running process, the OS will execute some low-level assembly code to save the general-purpose registers, PC, and the kernel stack pointer of the currently-running process, and then restore said registers, PC, and switch to the kernel stack for the soon-to-be-executing process. By switching stacks, the kernel enters the call to the switch code in the context of one process (the one that was interrupted) and returns in the context of another (the soon-to-be-executing one). When the OS then finally executes a return-from-trap instruction, the soon-to-be-executing process becomes the currently-running process. And thus the context switch is complete.\nIn this example, Process A is running and then is interrupted by the timer interrupt. The hardware saves its registers (onto its kernel stack) and enters the kernel (switching to kernel mode). In the timer interrupt handler, the OS decides to switch from running Process A to Process B. At that point, it calls the switch() routine, which carefully saves current register values (into the process structure of A), restores the registers of Process B (from its process structure entry), and then switches contexts, specifically by changing the stack pointer to use B’s kernel stack (and not A’s). Finally, the OS returns-from-trap, which restores B’s registers and starts running it.\nNote that there are two types of register saves/restores that happen during this protocol. The first is when the timer interrupt occurs; in this case, the user registers of the running process are implicitly saved by the hardware, using the kernel stack of that process. The second is when the OS decides to switch from A to B; in this case, the kernel registers are explicitly saved by the software (i.e., the OS), but this time into memory in the process structure of the process. The latter action moves the system from running as if it just trapped into the kernel from A to as if it just trapped into the kernel from B.\nTo give you a better sense of how such a switch is enacted, the code snippet below shows the context switch code for xv6:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # void swtch(struct context **old, struct context *new);  # # Save current register context in old # and then load register context from new. .globl swtch swtch: # Save old registers movl 4(%esp), %eax # put old ptr into eax popl 0(%eax) # save the old IP movl %esp, 4(%eax) # and stack movl %ebx, 8(%eax) # and other registers movl %ecx, 12(%eax) movl %edx, 16(%eax) movl %esi, 20(%eax) movl %edi, 24(%eax) movl %ebp, 28(%eax) # Load new registers movl 4(%esp), %eax # put new ptr into eax movl 28(%eax), %ebp # restore other registers movl 24(%eax), %edi movl 20(%eax), %esi movl 16(%eax), %edx movl 12(%eax), %ecx movl 8(%eax), %ebx movl 4(%eax), %esp # stack is switched here pushl 0(%eax). # return addr put in place ret # finally return into new ctxt   Worried About Concurrency? Disabling interrupts One simple thing an OS might do is disable interrupts during interrupt processing; doing so ensures that when one interrupt is being handled, no other one will be delivered to the CPU. Of course, the OS has to be careful in doing so; disabling interrupts for too long could lead to lost interrupts, which is (in technical terms) bad.\nLocking schemes Operating systems also have developed a number of sophisticated locking schemes to protect concurrent access to internal data structures. This enables multiple activities to be on-going within the kernel at the same time, particularly useful on multiprocessors.\nSummary We have described some key low-level mechanisms to implement CPU virtualization, a set of techniques that we collectively refer to as limited direct execution. The basic idea is straightforward: just run the program you want to run on the CPU, but first, make sure to set up the hardware so as to limit what the process can do without OS assistance.\nASIDE: KEY CPU VIRTUALIZATION TERMS (MECHANISMS)\n The CPU should support at least two modes of execution: a restricted user mode and a privileged (non-restricted) kernel mode. Typical user applications run in user mode, and use a system call to trap into the kernel to request operating system services. The trap instruction saves the register state carefully, changes the hardware status to kernel mode, and jumps into the OS to a pre-specified destination: the trap table. When the OS finishes servicing a system call, it returns to the user program via another special return-from-trap instruction, which reduces privilege and returns control to the instruction after the trap that jumped into the OS. The trap tables must be set up by the OS at boot time, and make sure that they cannot be readily modified by user programs. All of this is part of the limited direct execution protocol which runs programs efficiently but without loss of OS control. Once a program is running, the OS must use hardware mechanisms to ensure the user program does not run forever, namely the timer interrupt. This approach is a non-cooperative approach to CPU scheduling. Sometimes the OS, during a timer interrupt or system call, might wish to switch from running the current process to a different one, a low-level technique known as a context switch.  Virtualization: CPU Scheduling  How should we develop a basic framework for thinking about scheduling policies? What are the key assumptions? What metrics are important? What basic approaches have been used in the earliest of computer systems?\n Workload Assumptions and Scheduling Metrics Workload assumptions Let us first make a number of simplifying assumptions about the processes running in the system, sometimes collectively called the workload.\nWe will make the following assumptions about the processes, sometimes called jobs, that are running in the system:\n Each job runs for the same amount of time. All the jobs arrive at the same time. Once started, each job runs to completion. All jobs only use the CPU (i.e., they perform no I/O). The run-time of each job is known.  Scheduling metrics A metric is just something that we use to measure something, and there are a number of different metrics that make sense in scheduling.\nThe turnaround time of a job is defined as the time at which the job completes minus the time at which the job arrived in the system. More formally, the turnaround time Tturnaround is:\nTturnaround == Tcompletion − Tarrival\nFirst In, First Out (FIFO)  Assume also that each job runs for 10 seconds.  From the figure above, you can see that A finished at 10, B at 20, and C at 30. Thus, the average turnaround time for the three jobs is:\n$$\n\\frac{10+20+30}{3}=20\n$$\n In particular, let’s again assume three jobs (A, B, and C), but this time A runs for 100 seconds while B and C run for 10 each.  the average turnaround time for the three jobs is:\n$$\n\\frac{100+110+120}{3}=110\n$$\nConvoy effect The convoy effect, where a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer.\nShortest Job First (SJF) It runs the shortest job first, then the next shortest, and so on.\nthe average turnaround time for the three jobs is:\n$$\n\\frac{10+20+120}{3}=50\n$$\nHere we can illustrate the problem again with an example. This time, assume A arrives at t = 0t=0 and needs to run for 100 seconds, whereas B and C arrive at t = 10t=10 and each needs to run for 10 seconds. With pure SJF, we’d get the schedule seen in the figure below.\nShortest Time-to-Completion First (STCF) To address this concern, we need to relax assumption 3 (that jobs must run to completion), so let’s do that. We also need some machinery within the scheduler itself. As you might have guessed, given our previous discussion about timer interrupts and context switching, the scheduler can certainly do something else when B and C arrive: it can preempt job A and decide to run another job, perhaps continuing A later. SJF by our definition is a non-preemptive scheduler and thus suffers from the problems described before.\nthe average turnaround time for the three jobs is:\n$$\n\\frac{(120−0)+(20−10)+(30−10)}{3}=50\n$$\nA New Metric: Response Time In fact, for a number of early batch computing systems, these types of scheduling algorithms made some sense. However, the introduction of time-shared machines changed all that. Now users would sit at a terminal and demand interactive performance from the system as well. And thus, a new metric was born: response time.\nWe define response time as the time from when the job arrives in a system to the first time it is scheduled.\nMore formally: Tresponse == Tfirstrun − Tarrival\nFor example, if we had the schedule above (with A arriving at time 0, and B and C at time 10), the response time of each job is as follows: 0 for job A, 0 for B, and 10 for C (average: 3.33).\nApproaches not sensitive to response time As you might be thinking, STCF and related disciplines are not particularly good for response time. If three jobs arrive at the same time, for example, the third job has to wait for the previous two jobs to run in their entirety before being scheduled just once. While great for turnaround time, this approach is quite bad for response time and interactivity. Indeed, imagine sitting at a terminal, typing, and having to wait 10 seconds to see a response from the system just because some other job got scheduled in front of yours: not too pleasant.\nRound Robin The basic idea is simple: instead of running jobs to completion, RR runs a job for a time slice (sometimes called a scheduling quantum) and then switches to the next job in the run queue. For this reason, RR is sometimes called time-slicing. Note that the length of a time slice must be a multiple of the timer-interrupt period; thus if the timer interrupts every 10 milliseconds, the time slice could be 10, 20, or any other multiple of 10 ms.\nAssume three jobs AA, BB, and CC arrive at the same time in the system, and that they each wish to run for 5 seconds. SJF Again (Bad for Response Time)\nIn contrast, RR with a time-slice of 1 second would cycle through the jobs quickly. Check out the figure below:\nThe average response time of RR is: $\\frac{0+1+2}3=1$ ; for SJF, average response time is: $\\frac{0+5+10}3=5$.\nAs you can see, the length of the time slice is critical for RR. The shorter it is, the better the performance of RR under the response-time metric. However, making the time slice too short is problematic: suddenly the cost of context switching will dominate overall performance. Thus, deciding on the length of the time slice presents a trade-off to a system designer, making it long enough to amortize the cost of switching without making it so long that the system is no longer responsive.\nNote that the cost of context switching does not arise solely from the OS actions of saving and restoring a few registers. When programs run, they build up a great deal of state in CPU caches, TLBs, branch predictors, and other on-chip hardware. Switching to another job causes this state to be flushed and a new state relevant to the currently-running job to be brought in, which may exact a noticeable performance cost.\nIncorporating I/O A scheduler clearly has a decision to make when a job initiates an I/O request, because the currently-running job won’t be using the CPU during the I/O; it is blocked waiting for I/O completion. If the I/O is sent to a hard disk drive, the process might be blocked for a few milliseconds or longer, depending on the current I/O load of the drive. Thus, the scheduler should probably schedule another job on the CPU at that time.\nA and BB, which each need 50 ms of CPU time. However, there is one obvious difference: AA runs for 10 ms and then issues an I/O request (assume here that I/Os each take 10ms ), whereas BB simply uses the CPU for 50 ms and performs no I/O. The scheduler runs AA first, then BB after (see the figure below).\nTreating jobs as independent A common approach is to treat each 10-ms sub-job of AA as an independent job.\nWe see how a scheduler might incorporate I/O. By treating each CPU burst as a job, the scheduler makes sure processes that are “interactive” get run frequently. While those interactive jobs are performing I/O, other CPU-intensive jobs run, thus better utilizing the processor.\nNo more Oracle With a basic approach to I/O in place, we come to our final assumption: that the scheduler knows the length of each job. As we said before, this is likely the worst assumption we could make. In fact, in a general-purpose OS (like the ones we care about), the OS usually knows very little about the length of each job. So, how can we build an approach that behaves like SJF/STCF without such a priori knowledge? Further, how can we incorporate some of the ideas we have seen with the RR scheduler so that response time is also quite good?\nVirtualization: Multi-Level Feedback The Multi-level Feedback Queue (MLFQ) scheduler was first described by Corbato et al. in 1962 in a system known as the Compatible Time-Sharing System (CTSS).\n THE CRUX: HOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE?\nHow can we design a scheduler that both minimizes response time for interactive jobs while also minimizing turnaround time without a priori knowledge of job length?\n MLFQ: Basic Rules Priority-based rules In our treatment, the MLFQ has a number of distinct queues, each assigned a different priority level. At any given time, a job that is ready to run is in a single queue. MLFQ uses priorities to decide which job should run at a given time: a job with higher priority (i.e., a job on a higher queue) is chosen to run. Of course, more than one job may be in a given queue and thus have the same priority. In this case, we will just use round-robin scheduling among those jobs.\nThus, we arrive at the first two basic rules for MLFQ:\n Rule 1: If Priority(A) \u0026gt; Priority(B), A runs (B doesn’t). Rule 2: If Priority(A) = Priority(B), A \u0026amp; B run in RR.  The key to MLFQ scheduling, therefore, lies in how the scheduler sets priorities. Rather than giving a fixed priority to each job, MLFQ varies the priority of a job based on its observed behavior. If, for example, a job repeatedly relinquishes the CPU while waiting for input from the keyboard, MLFQ will keep its priority high, as this is how an interactive process might behave. If instead, a job uses the CPU intensively for long periods of time, MLFQ will reduce its priority. In this way, MLFQ will try to learn about processes as they run, and thus use the history of the job to predict its future behavior.\nAttempt #1: How To Change Priority Here is our first attempt at a priority-adjustment algorithm:\n Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue). Rule 4a: If a job uses up an entire time slice while running, its priority is reduced (i.e., it moves down one queue). Rule 4b: If a job gives up the CPU before the time slice is up, it stays at the same priority level.  Example 1: a single long-running job Let’s look at some examples. First, we’ll look at what happens when there has been a long-running job in the system. The figure below shows what happens to this job over time in a three-queue scheduler.\nAs you can see in the example, the job enters at the highest priority (Q2). After a single time-slice of 10 ms, the scheduler reduces the job’s priority by one, and thus the job is on Q1. After running at Q1 for a time slice, the job is finally lowered to the lowest priority in the system (Q0), where it remains. Pretty simple, no?\nExample 2: along came a short job The figure provided below plots the results of this scenario. A (shown in black) is running along in the lowest-priority queue (as would any long-running CPU-intensive jobs); B (shown in gray) arrives at time T = 100 and thus is inserted into the highest queue; as its run-time is short (only 20 ms), B completes before reaching the bottom queue, in two time slices; then A resumes running (at low priority).\nFrom this example, you can hopefully understand one of the major goals of the algorithm: because it doesn’t know whether a job will be a short job or a long-running job, it first assumes it might be a short job, thus giving the job high priority. If it actually is a short job, it will run quickly and complete; if it is not a short job, it will slowly move down the queues, and thus soon prove itself to be a long-running more batch-like process. In this manner, MLFQ approximates SJF.\nExample 3: what about I/O? The intent of this rule is simple: if an interactive job, for example, is doing a lot of I/O (say by waiting for user input from the keyboard or mouse), it will relinquish the CPU before its time slice is complete; in such case, we don’t wish to penalize the job and thus simply keep it at the same level.\nThe figure below shows an example of how this works, with an interactive job B (shown in gray) that needs the CPU only for 1 ms before performing an I/O competing for the CPU with a long-running batch job A (shown in black). The MLFQ approach keeps B at the highest priority because B keeps releasing the CPU; if B is an interactive job, MLFQ further achieves its goal of running interactive jobs quickly.\nProblems with our current MLFQ First, there is the problem of starvation: if there are “too many” interactive jobs in the system, they will combine to consume all CPU time, and thus long-running jobs will never receive any CPU time (they starve). We’d like to make some progress on these jobs even in this scenario.\nSecond, a smart user could rewrite their program to game the scheduler. Gaming the scheduler generally refers to the idea of doing something sneaky to trick the scheduler into giving you more than your fair share of the resource. The algorithm we have described is susceptible to the following attack: before the time slice is over, issue an I/O operation (to some file you don’t care about) and thus relinquish the CPU; doing so allows you to remain in the same queue, and thus gain a higher percentage of CPU time. When done right (e.g., by running for 99% of a time slice before relinquishing the CPU), a job could nearly monopolize the CPU.\nFinally, a program may change its behavior over time; what was CPU- bound may transition to a phase of interactivity. With our current approach, such a job would be out of luck and not be treated like the other interactive jobs in the system.\nAttempt #2: The Priority Boost Preventing starvation  Rule 5: After some time period S, move all the jobs in the system to the topmost queue.  Our new rule solves two problems at once. First, processes are guaranteed not to starve: by sitting in the top queue, a job will share the CPU with other high-priority jobs in a round-robin fashion, and thus eventually receive service. Second, if a CPU-bound job has become interactive, the scheduler treats it properly once it has received the priority boost.\nOn the left, there is no priority boost, and thus the long-running job gets starved once the two short jobs arrive; on the right, there is a priority boost every 50 ms (which is likely too small of a value, but used here for the example), and thus we at least guarantee that the long-running job will make some progress, getting boosted to the highest priority every 50 ms and thus getting to run periodically.\nWhat should S be set to?Unfortunately, S has that flavor. If it is set too high, long-running jobs could starve; too low, and interactive jobs may not get a proper share of the CPU.\nAttempt #3: Better Accounting We now have one more problem to solve: how to prevent gaming of our scheduler?\nThe solution here is to perform better accounting of CPU time at each level of the MLFQ. Instead of forgetting how much of a time slice a process used at a given level, the scheduler should keep track; once a process has used its allotment, it is demoted to the next priority queue. Whether it uses the time slice in one long burst or many small ones does not matter. We thus rewrite Rules 4a and 4b to the following single rule:\n Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).  Without (Left) and With (Right) Gaming Tolerance:\nWithout any protection from gaming, a process can issue an I/O just before a time slice ends and thus dominate CPU time. With such protections in place, regardless of the I/O behavior of the process, it slowly moves down the queues, and thus cannot gain an unfair share of the CPU.\nTuning MLFQ And Other Issues Parameterizing the scheduler A few other issues arise with MLFQ scheduling. One big question is how to parameterize such a scheduler. For example, how many queues should there be? How big should the time slice be per queue? How often should priority be boosted in order to avoid starvation and account for changes in behavior? There are no easy answers to these questions, and thus only some experience with workloads and subsequent tuning of the scheduler will lead to a satisfactory balance.\nFor example, most MLFQ variants allow for varying time-slice length across different queues. The high-priority queues are usually given short time slices; they consist of interactive jobs, after all, and thus quickly alternating between them makes sense (e.g., 10 or fewer milliseconds). The low-priority queues, in contrast, contain long-running jobs that are CPU-bound; hence, longer time slices work well (e.g., 100s of ms).\nThe figure below shows an example in which two jobs run for 20 ms at the highest queue (with a 10-ms time slice), 40 ms in the middle (20-ms time slice), and with a 40-ms time slice at the lowest.\nOther variants of MLFQ Default values for the table are 60 queues, with slowly increasing time-slice lengths from 20 milliseconds (highest priority) to a few hundred milliseconds (lowest), and priorities boosted around every 1 second or so.\nOther MLFQ schedulers don’t use a table or the exact rules described in this chapter; rather they adjust priorities using mathematical formulae.\nFinally, many schedulers have a few other features that you might encounter. For example, some schedulers reserve the highest priority levels for operating system work; thus typical user jobs can never obtain the highest levels of priority in the system. Some systems also allow some user advice to help set priorities; for example, by using the command-line utility, you can increase or decrease the priority of a job (somewhat) and thus increase or decrease its chances of running at any given time. See the man page for more.\nSummary The refined set of MLFQ rules, spread throughout the chapter, are reproduced here for your viewing pleasure:\n Rule 1: If Priority(A) \u0026gt; Priority(B), A runs (B doesn’t). Rule 2: If Priority(A) = Priority(B), A \u0026amp; B run in round-robin fashion using the time slice (quantum length) of the given queue. Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue). Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue). Rule 5: After some time period S, move all the jobs in the system to the topmost queue.  MLFQ is interesting for the following reason: instead of demanding a priori knowledge of the nature of a job, it observes the execution of a job and prioritizes it accordingly. In this way, it manages to achieve the best of both worlds: it can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs, and is fair and makes progress for long-running CPU-intensive workloads.\nVirtualization: Lottery Scheduling Proportional-share, also sometimes referred to as a fair-share scheduler, is based around a simple concept: instead of optimizing for turnaround or response time, a scheduler might instead try to guarantee that each job obtains a certain percentage of CPU time.\nBasic Concept: Tickets Represent Your Share Underlying lottery scheduling is one very basic concept: tickets, which are used to represent the share of a resource that a process (or user or whatever) should receive. The percent of tickets that a process has represents its share of the system resource in question.\nAssuming AA holds tickets 0 through 74 and BB 75 through 99, the winning ticket simply determines whether AA or BB runs. The scheduler then loads the state of that winning process and runs it.\n TIP: USE RANDOMNESS\nOne of the most beautiful aspects of lottery scheduling is its use of randomness. When you have to make a decision, using such a randomized approach is often a robust and simple way of doing so.\nRandom approaches have at least three advantages over more traditional decisions. First, random often avoids strange corner-case behaviors that a more traditional algorithm may have trouble handling. For example, consider the LRU replacement policy (studied in more detail in a future chapter on virtual memory); while often a good replacement algorithm, LRU attains worst-case performance for some cyclic-sequential workloads. Random, on the other hand, has no such worst case.\nSecond, random also is lightweight, requiring little state to track alternatives. In a traditional fair-share scheduling algorithm, tracking how much CPU each process has received requires per-process accounting, which must be updated after running each process. Doing so randomly necessitates only the most minimal of per-process state (e.g., the number of tickets each has).\nFinally, random can be quite fast. As long as generating a random number is quick, making the decision is also, and thus random can be used in a number of places where speed is required. Of course, the faster the need, the more random tends towards pseudo-random.\n  TIP: USE TICKETS TO REPRESENT SHARES\nOne of the most powerful (and basic) mechanisms in the design of lottery (and stride) scheduling is that of the ticket. The ticket is used to represent a process’s share of the CPU in these examples but can be applied much more broadly. For example, in more recent work on virtual memory management for hypervisors, Waldspurger shows how tickets can be used to represent a guest operating system’s share of memory. Thus, if you are ever in need of a mechanism to represent a proportion of ownership, this concept just might be … (wait for it) … the ticket.\n Ticket Mechanisms Ticket currency Lottery scheduling also provides a number of mechanisms to manipulate tickets in different and sometimes useful ways. One way is with the concept of ticket currency. Currency allows a user with a set of tickets to allocate tickets among their own jobs in whatever currency they would like; the system then automatically converts said currency into the correct global value.\nFor example, assume users A and B have each been given 100 tickets. User A is running two jobs, A1 and A2, and gives them each 500 tickets (out of 1000 total) in A’s currency. User B is running only 1 job and gives it 10 tickets (out of 10 total).\n1 2 3  User A -\u0026gt; 500 (A’s currency) to A1 -\u0026gt; 50 (global currency) -\u0026gt; 500 (A’s currency) to A2 -\u0026gt; 50 (global currency) User B -\u0026gt; 10 (B’s currency) to B1 -\u0026gt; 100 (global currency)   Ticket transfer Another useful mechanism is ticket transfer. With transfers, a process can temporarily hand off its tickets to another process. This ability is especially useful in a client/server setting, where a client process sends a message to a server asking it to do some work on the client’s behalf. To speed up the work, the client can pass the tickets to the server and thus try to maximize the performance of the server while the server is handling the client’s request. When finished, the server then transfers the tickets back to the client and all is as before.\nTicket inflation Finally, ticket inflation can sometimes be a useful technique. With inflation, a process can temporarily raise or lower the number of tickets it owns. Of course, in a competitive scenario with processes that do not trust one another, this makes little sense; one greedy process could give itself a vast number of tickets and take over the machine. Rather, inflation can be applied in an environment where a group of processes trusts one another; in such a case, if anyone process knows it needs more CPU time, it can boost its ticket value as a way to reflect that need to the system, all without communicating with any other processes.\nImplementation Let’s assume we keep the processes in a list. Here is an example of three processes, A, B, and C, each with some number of tickets.\nTo make a scheduling decision, we first have to pick a random number (the winner) from the total number of tickets (400). Let’s say we pick the number 300. Then, we simply traverse the list, with a simple counter used to help us find the winner. Have a look at the code below:\nLottery Scheduling Decision Code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // counter: used to track if we’ve found the winner yet int counter = 0; // winner: use some call to a random number generator to // get a value, between 0 and the total # of tickets  int winner = getrandom(0, totaltickets); // current: use this to walk through the list of jobs node_t *current = head; while (current) { counter = counter + current-\u0026gt;tickets; if (counter \u0026gt; winner) break; // found the winner  current = current-\u0026gt;next; } // ’current’ is the winner: schedule it... }   The code walks the list of processes, adding each ticket value to counter until the value exceeds winner. Once that is the case, the current list element is the winner. With our example of the winning ticket being 300, the following takes place. First, counter is incremented to 100 to account for A’s tickets; because 100 is less than 300, the loop continues. Then counter would be updated to 150 (B’s tickets), still less than 300 and thus again we continue. Finally, counter is updated to 400 (clearly greater than 300), and thus we break out of the loop with current pointing at C (the winner).\nTo make this process the most efficient, it might generally be best to organize the list in sorted order, from the highest number of tickets to the lowest. The ordering does not affect the correctness of the algorithm; however, it does ensure in general that the fewest number of list iterations are taken, especially if there are a few processes that possess most of the tickets.\nUnfairness metric We’d like for each job to finish at roughly the same time, but due to the randomness of lottery scheduling, sometimes one job finishes before the other. To quantify this difference, we define a simple unfairness metric, UU which is simply the time the first job completes divided by the time that the second job completes. For example, if R = 10R=10, and the first job finishes at time 10 (and the second job at 20), $U = \\frac {10}{20} =0.5$. When both jobs finish at nearly the same time, U will be 20 quite close to 1. In this scenario, that is our goal: a perfectly fair scheduler would achieve U=1.\nAs you can see from the graph, when the job length is not very long, the average unfairness can be quite severe. Only as the jobs run for a significant number of time slices does the lottery scheduler approach the desired outcome.\nHow to assign tickets? However, this solution is a non-solution: it really doesn’t tell you what to do. Thus, given a set of jobs, the “ticket-assignment problem” remains open.\nWhy Not Deterministic? As we saw above, while randomness gets us a simple (and approximately correct) scheduler, it occasionally will not deliver the exact right proportions, especially over short time scales. For this reason, Waldspurger invented stride scheduling, a deterministic fair-share scheduler.\nStride scheduling Stride scheduling is also straightforward. Each job in the system has a stride, which is inverse in proportion to the number of tickets it has. In the example discussed previously, with jobs A, B, and C, with 100, 50, and 250 tickets, respectively, we can compute the stride of each by dividing some large number by the number of tickets each process has been assigned. For example, if we divide 10,000 by each of those ticket values, we obtain the following stride values for A, B, and C: 100, 200, and 40. We call this value the stride of each process; every time a process runs, we will increment a counter for it (called its pass value) by its stride to track its global progress.\nThe scheduler then uses the stride and pass to determine which process should run next. The basic idea is simple: at any given time, pick the process to run that has the lowest pass value so far; when you run a process, increment its pass counter by its stride. A pseudocode implementation is provided by Waldspurger:\n1 2 3 4  curr = remove_min(queue); // pick client with min pass schedule(curr); // run for quantum curr-\u0026gt;pass += curr-\u0026gt;stride; // update pass using stride insert(queue, curr); // return curr to queue   Example In our example, we start with three processes (A, B, and C), with stride values of 100, 200, and 40, and all with pass values initially at 0. Thus, at first, any of the processes might run, as their pass values are equally low. Assume we pick A (arbitrarily; any of the processes with equal low pass values can be chosen). A runs; when finished with the time slice, we update its pass value to 100. Then we run B, whose pass value is then set to 200. Finally, we run C, whose pass value is incremented to 40. At this point, the algorithm will pick the lowest pass value, which is C’s, and run it, updating its pass to 80 (C’s stride is 40, as you recall). Then C will run it, updating run again (still the lowest pass value), raising its pass to 120. A will run now, updating its pass to 200 (now equal to B’s). Then C will run twice more, updating its pass to 160 then 200. At this point, all pass values are equal again, and the process will repeat, ad infinitum.\nThe figure below traces the behavior of the scheduler over time.\nLottery scheduling achieves the proportions probabilistically over time; stride scheduling gets them exactly right at the end of each scheduling cycle.\nSo you might be wondering: given the precision of stride scheduling, why use lottery scheduling at all? Well, lottery scheduling has one nice property that stride scheduling does not: no global state. Imagine a new job enters in the middle of our stride scheduling example above; what should its pass value be? Should it be set to 0? If so, it will monopolize the CPU. With lottery scheduling, there is no global state per process; we simply add a new process with whatever tickets it has, update the single global variable to track how many total tickets we have and go from there. In this way, the lottery makes it much easier to incorporate new processes in a sensible manner.\n而步长调度的缺点则是，在新进程加入时，它的行程值为0，会使得它长时间独占CPU\nThe Linux Completely Fair Scheduler (CFS) The scheduler, entitled the Completely Fair Scheduler (or CFS), implements fair-share scheduling but does so in a highly efficient and scalable manner. Reducing that overhead as much as possible is thus a key goal in modern scheduler architecture.\nBasic operation Whereas most schedulers are based around the concept of a fixed time slice, CFS operates a bit differently. Its goal is simple: to fairly divide a CPU evenly among all competing processes. It does so through a simple counting-based technique known as virtual runtime (vruntime).\nAs each process runs, it accumulates vruntime. In the most basic case, each process’s vruntime increases at the same rate, in proportion with physical (real) time. When a scheduling decision occurs, CFS will pick the process with the lowest vruntime to run next.\nThis raises a question: how does the scheduler know when to stop the currently running process, and run the next one? The tension here is clear: if CFS switches too often, fairness is increased, as CFS will ensure that each process receives its share of CPU even over minuscule time windows, but at the cost of performance (too much context switching); if CFS switches less often, performance is increased (reduced context switching), but at the cost of near-term fairness.\nThe sched_latency parameter CFS manages this tension through various control parameters. The first is sched_latency. CFS uses this value to determine how long one process should run before considering a switch (effectively determining its time slice but in a dynamic fashion). A typical sched_latency value is 48 (milliseconds); CFS divides this value by the number (n) of processes running on the CPU to determine the time slice for a process, and thus ensures that over this period of time, CFS will be completely fair.\nExample The figure below shows an example where the four jobs (A, B, C, D) each run for two time slices in this fashion; two of them (C, D) then complete, leaving just two remaining, which then each run for 24 ms in round-robin fashion.\nBut what if there are “too many” processes running? Wouldn’t that lead to too small of a time slice, and thus too many context switches? Good question! And the answer is yes.\nThe min_granularity parameter To address this issue, CFS adds another parameter, min_granularity, which is usually set to a value like 6 ms. CFS will never set the time slice of a process to less than this value, ensuring that not too much time is spent in scheduling overhead.\nNote that CFS utilizes a periodic timer interrupt, which means it can only make decisions at fixed time intervals. This interrupt goes off frequently (e.g., every 1 ms), giving CFS a chance to wake up and determine if the current job has reached the end of its run. If a job has a time slice that is not a perfect multiple of the timer interrupt interval, that is OK; CFS tracks vruntime precisely, which means that over the long haul, it will eventually approximate ideal sharing of the CPU.\nNice level of a process CFS also enables controls over process priority, enabling users or administrators to give some processes a higher share of the CPU. It does this not with tickets, but through a classic UNIX mechanism known as the nice level of a process. The nice parameter can be set anywhere from -20 to +19 for a process, with a default of 0. Positive nice values imply lower priority and negative values imply higher priority; when you’re too nice, you just don’t get as much (scheduling) attention, alas.\nCFS maps the nice value of each process to a weight, as shown here:\n1 2 3 4 5 6 7 8 9 10  static const int prio_to_weight[40] = { /* -20 */ 88761, 71755, 56483, 46273, 36291, /* -15 */ 29154, 23254, 18705, 14949, 11916, /* -10 */ 9548, 7620, 6100, 4904, 3906, /* -5 */ 3121, 2501, 1991, 1586, 1277, /* 0 */ 1024, 820, 655, 526, 423, /* 5*/ 335, 272, 215, 172, 137, /* 10 */ 110, 87, 70, 56, 45, /*15*/ 36, 29, 23, 18, 15, };   These weights allow us to compute the effective time slice of each process (as we did before), but now accounting for their priority differences. The formula used to do so is as follows:\n$$\ntimeslice_k = \\frac {weight_k}{\\sum_{i=0}^{n-1} {weight_i}}\n· sched_latency\n$$\n","description":"\u003c操作系统\u003e个人摘录笔记, 整体文档框架没变, 原课程在educative.io","id":3,"section":"posts","tags":["Operating Systems"],"title":"Operating Systems: Virtualization, Concurrency \u0026 Persistence","uri":"/en/posts/operating-systems-virtualization-concurrency-persistence/"},{"content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  spring: datasource: url: jdbc:mysql://localhost:3306/couurse?serverTimezone=UTC driver-class-name: com.mysql.jdbc.Driver username: course password: root redis: host: 127.0.0.1 port: 6379 # password: \u0026#34;test123\u0026#34; mybatis: type-aliases-package: com.course.system.domain configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl mapper-locations: classpath:/mapper/**/*.xml   ","description":"Mybatis日志开启yml设置","id":4,"section":"posts","tags":[],"title":"Mybatis日志开启yml设置","uri":"/en/posts/course-online-project/mybatis-yml/"},{"content":"导入jar包\n1 2 3 4 5  \u0026lt;!-- 热部署DevTools --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-devtools\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;   idea 设置项目自动编译\nidea双击shift键, 搜索registry\n","description":"springboot集成热部署DevTools","id":5,"section":"posts","tags":["SpringBoot"],"title":"springboot集成热部署DevTools","uri":"/en/posts/course-online-project/springboot-devtools/"},{"content":" 主要针对单表的增删改查, 只需要很少量的简单配置，就可以完成大量的表到Java对象的生成工作，能快捷的创建好Dao，entry，xml 加快了开发速度，拥有零出错和速度快的优点，让开发人员解放出来更专注于业务逻辑的开发。\n 父pom.xml里增加插件 mybatis-generator插件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;!-- mybatis generator 自动生成代码插件 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis.generator\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-generator-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.3.7\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;configurationFile\u0026gt;src/main/resources/generator/generatorConfig.xml\u0026lt;/configurationFile\u0026gt; \u0026lt;overwrite\u0026gt;true\u0026lt;/overwrite\u0026gt; \u0026lt;verbose\u0026gt;true\u0026lt;/verbose\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.37\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt;   新增generatorConfig.xml配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE generatorConfiguration PUBLIC \u0026#34;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\u0026#34;\u0026gt; \u0026lt;generatorConfiguration\u0026gt; \u0026lt;context id=\u0026#34;Mysql\u0026#34; targetRuntime=\u0026#34;MyBatis3\u0026#34; defaultModelType=\u0026#34;flat\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;autoDelimitKeywords\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;beginningDelimiter\u0026#34; value=\u0026#34;`\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;endingDelimiter\u0026#34; value=\u0026#34;`\u0026#34;/\u0026gt; \u0026lt;!--覆盖生成XML文件--\u0026gt; \u0026lt;plugin type=\u0026#34;org.mybatis.generator.plugins.UnmergeableXmlMappersPlugin\u0026#34; /\u0026gt; \u0026lt;!-- 生成的实体类添加toString()方法 --\u0026gt; \u0026lt;plugin type=\u0026#34;org.mybatis.generator.plugins.ToStringPlugin\u0026#34; /\u0026gt; \u0026lt;!-- 不生成注释 --\u0026gt; \u0026lt;commentGenerator\u0026gt; \u0026lt;property name=\u0026#34;suppressAllComments\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;/commentGenerator\u0026gt; \u0026lt;jdbcConnection driverClass=\u0026#34;com.mysql.jdbc.Driver\u0026#34; connectionURL=\u0026#34;jdbc:mysql://localhost:3306/couurse\u0026#34; userId=\u0026#34;course\u0026#34; password=\u0026#34;root\u0026#34;\u0026gt; \u0026lt;/jdbcConnection\u0026gt; \u0026lt;!-- domain类的位置 --\u0026gt; \u0026lt;javaModelGenerator targetProject=\u0026#34;src\\main\\java\u0026#34; targetPackage=\u0026#34;com.course.server.domain\u0026#34;/\u0026gt; \u0026lt;!-- mapper xml的位置 --\u0026gt; \u0026lt;sqlMapGenerator targetProject=\u0026#34;src\\main\\resources\u0026#34; targetPackage=\u0026#34;mapper\u0026#34;/\u0026gt; \u0026lt;!-- mapper类的位置 --\u0026gt; \u0026lt;javaClientGenerator targetProject=\u0026#34;src\\main\\java\u0026#34; targetPackage=\u0026#34;com.course.server.mapper\u0026#34; type=\u0026#34;XMLMAPPER\u0026#34; /\u0026gt; \u0026lt;!-- 需要生成的表名 --\u0026gt; \u0026lt;!-- \u0026lt;table tableName=\u0026#34;test\u0026#34; domainObjectName=\u0026#34;Test\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!-- \u0026lt;table tableName=\u0026#34;chapter\u0026#34; domainObjectName=\u0026#34;Chapter\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;table tableName=\u0026#34;member_course\u0026#34; domainObjectName=\u0026#34;MemberCourse\u0026#34;/\u0026gt; \u0026lt;/context\u0026gt; \u0026lt;/generatorConfiguration\u0026gt;   Maven 启动命令 mybatis-generator:generate -e  Edit Configuration.. 中配置\n单表查询-Example使用 引入mybatis分页插件pagehelper 1 2 3 4 5 6  \u0026lt;!-- mybatis分页插件pagehelper --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.pagehelper\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;pagehelper-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.10\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   查询老师数据库总信息\n1 2 3 4 5 6  PageHelper.startPage(pageDto.getPage(),pageDto.getSize()); TeacherExample teacherExample = new TeacherExample(); List\u0026lt;Teacher\u0026gt; teachers = teacherMapper.selectByExample(teacherExample); PageInfo\u0026lt;Teacher\u0026gt; pageInfo = new PageInfo\u0026lt;\u0026gt;(teachers); pageDto.setTotal(pageInfo.getTotal());   管理老师信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  /** * 新增 */ private void insert(Teacher teacher) { teacher.setId(UuidUtil.getShortUuid()); teacherMapper.insert(teacher); } /** * 更新 */ private void update(Teacher teacher) { teacherMapper.updateByPrimaryKey(teacher); } /** * 删除 */ public void delete(String id) { teacherMapper.deleteByPrimaryKey(id); } /** * 查找 */ public TeacherDto findById(String id) { Teacher teacher = teacherMapper.selectByPrimaryKey(id); return CopyUtil.copy(teacher, TeacherDto.class); }   CopyUtil类 用于类的拷贝\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  public class CopyUtil { public static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; copyList(List source, Class\u0026lt;T\u0026gt; clazz) { List\u0026lt;T\u0026gt; target = new ArrayList\u0026lt;\u0026gt;(); if (!CollectionUtils.isEmpty(source)){ if (!CollectionUtils.isEmpty(source)){ for (Object c: source) { T obj = copy(c, clazz); target.add(obj); } } } return target; } public static \u0026lt;T\u0026gt; T copy(Object source, Class\u0026lt;T\u0026gt; clazz) { if (source == null) { return null; } T obj = null; try { obj = clazz.newInstance(); } catch (Exception e) { e.printStackTrace(); } BeanUtils.copyProperties(source, obj); return obj; } }   ","description":"","id":6,"section":"posts","tags":["Mybatis"],"title":"SpringBoot项目集成mybatis generator\t","uri":"/en/posts/course-online-project/springboot-mybatis-generator/"},{"content":"后端spring boot启动配置   spring boot 多环境配置 使用application-xxx.yml来设置不同的配置, xxx表示环境名 公共模块打包, 不能带spring-boot-maven-plugin插件 springboot jar包启动命令: java -jar system-0.0.1-SNAPSHOT.jar \u0026ndash;spring.profiles.active=xxx, xxx代表环境名   多环境配置 通过再启动命令里增加spring.profiles.active的变量值, 达到支持多环境的效果\n  复制配置文件改名为application-dev.yml\n  修改需要更改的环境配置\n  到 Edit Configurations.. 中去修改activeprofile的参数\n  重启springboot项目模块\n  连接数据库配置设置同理\n  打包 问题插曲1: process terminated\n在Maven中执行install出现process terminated 错误, 排查后是在pom.xml中忘记复制标签了:\n1 2 3 4 5 6 7 8  \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt;   打包成功:\n查看jar包:\n设置里面查看仓库地址:\n启动jar中的项目\n1 2  H3L1 MINGW64 /d/courseProject/day0410/course/system/target (master) $ java -jar system-0.0.1-SNAPSHOT.jar --spring.profiles.active=dev   前端Vue的多环境配置 在项目根目录创建文件 //.env.dev文件 NODE_ENV=development VUE_APP_SERVER=http://127.0.0.1:9000 //.env.prod文件 NODE_ENV=production VUE_APP_SERVER=http://server.xxxxx.com  到package.json中配置:\n1 2 3 4 5 6 7 8 9 10 11 12  { \u0026#34;name\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;private\u0026#34;: true, \u0026#34;scripts\u0026#34;: { //这里配置下面两条启动命令 \u0026#34;serve-dev\u0026#34;: \u0026#34;vue-cli-service serve --mode dev\u0026#34;, \u0026#34;serve-prod\u0026#34;: \u0026#34;vue-cli-service serve --mode prod\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;vue-cli-service build\u0026#34;, \u0026#34;lint\u0026#34;: \u0026#34;vue-cli-service lint\u0026#34; },   右键点击package.json 进入npm窗口\n启动对应环境的命令就行了\n","description":"常见环境为: 开发, 测试, 集成, 联调, 准备生产, 灰度, 生产","id":7,"section":"posts","tags":["idea"],"title":"SpringBoot 前后端多环境配置","uri":"/en/posts/course-online-project/idea/"},{"content":"使用if条件语句, 实现按条件关联课程表course与课程分类表course_category\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34; \u0026gt; \u0026lt;mapper namespace=\u0026#34;com.course.server.mapper.my.MyCourseMapper\u0026#34; \u0026gt; \u0026lt;!-- web端【全部课程】页面，查询课程列表 --\u0026gt; \u0026lt;select id=\u0026#34;list\u0026#34; resultType=\u0026#34;com.course.server.dto.CourseDto\u0026#34;\u0026gt; select c.id, c.name, c.summary, time, price, image, level, charge, status, enroll, sort, created_at as createdAt, updated_at as updatedAt, teacher_id as teacherId from `course` c \u0026lt;if test=\u0026#34;pageDto.categoryId != null and pageDto.categoryId != \u0026#39;\u0026#39;\u0026#34;\u0026gt; , course_category cc \u0026lt;/if\u0026gt; where 1 = 1 \u0026lt;if test=\u0026#34;pageDto.categoryId != null and pageDto.categoryId != \u0026#39;\u0026#39;\u0026#34;\u0026gt; and c.id = cc.course_id and cc.category_id = #{pageDto.categoryId} \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;pageDto.status != null and pageDto.status != \u0026#39;\u0026#39;\u0026#34;\u0026gt; and c.status = #{pageDto.status} \u0026lt;/if\u0026gt; order by c.sort asc \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt;   ","description":"MyBatis动态SQL不只where的动态 也可以多表关联的动态","id":9,"section":"posts","tags":["Mybatis"],"title":"MyBatis动态SQL可以按条件进行多表关联","uri":"/en/posts/course-online-project/mybatis-sql/"},{"content":" 在运行 vue create web 指令时出现npm错误\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  added 1345 packages in 58s � Invoking generators... � Installing additional dependencies... npm ERR! code ERESOLVE npm ERR! ERESOLVE unable to resolve dependency tree npm ERR! npm ERR! Found: vue@2.6.12 npm ERR! node_modules/vue npm ERR! vue@\u0026#34;^2.6.11\u0026#34; from the root project npm ERR! peer vue@\u0026#34;^2 || ^3.0.0-0\u0026#34; from @vue/babel-preset-app@4.5.13 npm ERR! node_modules/@vue/babel-preset-app npm ERR! @vue/babel-preset-app@\u0026#34;^4.5.13\u0026#34; from @vue/cli-plugin-babel@4.5.13 npm ERR! node_modules/@vue/cli-plugin-babel npm ERR! dev @vue/cli-plugin-babel@\u0026#34;^4.5.0\u0026#34; from the root project npm ERR! npm ERR! Could not resolve dependency: npm ERR! peer vue@\u0026#34;3.0.11\u0026#34; from @vue/compiler-sfc@3.0.11 npm ERR! node_modules/@vue/compiler-sfc npm ERR! peer @vue/compiler-sfc@\u0026#34;^3.0.0-beta.14\u0026#34; from @vue/cli-service@4.5.13 npm ERR! node_modules/@vue/cli-service npm ERR! dev @vue/cli-service@\u0026#34;^4.5.0\u0026#34; from the root project npm ERR! 4 more (@vue/cli-plugin-babel, @vue/cli-plugin-eslint, ...) npm ERR! npm ERR! Fix the upstream dependency conflict, or retry npm ERR! this command with --force, or --legacy-peer-deps npm ERR! to accept an incorrect (and potentially broken) dependency resolution. npm ERR! npm ERR! See C:\\Users\\TANG\\AppData\\Local\\npm-cache\\eresolve-report.txt for a full report. npm ERR! A complete log of this run can be found in: npm ERR! C:\\Users\\TANG\\AppData\\Local\\npm-cache\\_logs\\2021-05-16T13_14_25_524Z-debug.log ERROR command failed: npm install --loglevel error --registry=https://registry.npm.taobao.org/ --registry=https://registry.npm.taobao.org/   按照提示操作不起作用\n重新安装vue-cli 卸载旧版本vue-cli 1  npm uninstall -g vue-cli   卸载时出现问题 需要检查C:\\Users\\用户\\.npmrc文件，删除以下代码\ncache=C:\\Program Files\\nodejs\\node_cache prefix=C:\\Program Files\\nodejs\\node_global 安装新版本vue-cli 1  npm install -g @vue/cli   再次创建项目 1  vue create web   如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客https://johntunliu.gitee.io/或微信公众号LiuJohntun，记录并分享我的所见、所学、所想\u0026hellip;\n","description":"在运行 vue create web 指令时出现npm错误, 重装Vue后恢复, 重装时遇到卸载不了Vue的问题","id":10,"section":"posts","tags":["vue"],"title":"执行命令vue create web报错以及卸载不了Vue cli的问题","uri":"/en/posts/course-online-project/vue-create-web/"},{"content":"权限管理分析 权限拦截的对象：用户\n权限拦截的点：菜单，路由，接口，按钮\n控制用户对资源的访问\n权限的操作：配置，读取，拦截\n核心概念：用户、角色、资源\n权限管理的功能点 配置：  用户管理：用户表，用户管理界面 资源配置：资源表，资源配置界面 角色管理：角色表，角色管理界面 用户角色关联配置：用户角色关联表，复用角色管理界面 角色资源关联配置：角色资源关联表，复用角色管理界面  读取：  用户权限的读取：用户登录时，读取该用户所有权限  拦截：  用户操作业务时，进行权限拦截  前端界面：菜单，路由，按钮，hidden disabled 后端接口：接口，gateway的过滤器    权限初始化 数据库表的创建 数据表的结构 用户表 1 2 3 4 5 6 7 8 9 10 11 12  drop table if exists `user`; create table `user` ( `id` char(8) not null default \u0026#39;\u0026#39; comment \u0026#39;id\u0026#39;, `login_name` varchar(50) not null comment \u0026#39;登陆名\u0026#39;, `name` varchar(50) comment \u0026#39;昵称\u0026#39;, `password` char(32) not null comment \u0026#39;密码\u0026#39;, primary key (`id`), unique key `login_name_unique` (`login_name`) ) engine=innodb default charset=utf8mb4 comment=\u0026#39;用户\u0026#39;; # 初始test/test insert into `user` (id, login_name, name, password) values (\u0026#39;10000000\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;测试\u0026#39;, \u0026#39;e70e2222a9d67c4f2eae107533359aa4\u0026#39;);   资源表 1 2 3 4 5 6 7 8 9 10 11  drop table if exists `resource`; create table `resource` ( `id` char(6) not null default \u0026#39;\u0026#39; comment \u0026#39;id\u0026#39;, `name` varchar(100) not null comment \u0026#39;名称|菜单或按钮\u0026#39;, `page` varchar(50) null comment \u0026#39;页面|路由\u0026#39;, `request` varchar(200) null comment \u0026#39;请求|接口\u0026#39;, `parent` char(6) comment \u0026#39;父id\u0026#39;, primary key (`id`) ) engine=innodb default charset=utf8mb4 comment=\u0026#39;资源\u0026#39;; insert into `resource` values (\u0026#39;01\u0026#39;, \u0026#39;系统管理\u0026#39;, null, null, null);   角色表 1 2 3 4 5 6 7 8 9 10 11  drop table if exists `role`; create table `role` ( `id` char(8) not null default \u0026#39;\u0026#39; comment \u0026#39;id\u0026#39;, `name` varchar(50) not null comment \u0026#39;角色\u0026#39;, `desc` varchar(100) not null comment \u0026#39;描述\u0026#39;, primary key (`id`) ) engine=innodb default charset=utf8mb4 comment=\u0026#39;角色\u0026#39;; insert into `role` values (\u0026#39;00000000\u0026#39;, \u0026#39;系统管理员\u0026#39;, \u0026#39;管理用户、角色权限\u0026#39;); insert into `role` values (\u0026#39;00000001\u0026#39;, \u0026#39;开发\u0026#39;, \u0026#39;维护资源\u0026#39;); insert into `role` values (\u0026#39;00000002\u0026#39;, \u0026#39;业务管理员\u0026#39;, \u0026#39;负责业务管理\u0026#39;);   用户角色关联表 1 2 3 4 5 6 7 8 9  drop table if exists `role_user`; create table `role_user` ( `id` char(8) not null default \u0026#39;\u0026#39; comment \u0026#39;id\u0026#39;, `role_id` char(8) not null comment \u0026#39;角色|id\u0026#39;, `user_id` char(8) not null comment \u0026#39;用户|id\u0026#39;, primary key (`id`) ) engine=innodb default charset=utf8mb4 comment=\u0026#39;角色用户关联\u0026#39;; insert into `role_user` values (\u0026#39;00000000\u0026#39;, \u0026#39;00000000\u0026#39;, \u0026#39;10000000\u0026#39;);   角色资源关联表 1 2 3 4 5 6 7 8 9  drop table if exists `role_resource`; create table `role_resource` ( `id` char(8) not null default \u0026#39;\u0026#39; comment \u0026#39;id\u0026#39;, `role_id` char(8) not null comment \u0026#39;角色|id\u0026#39;, `resource_id` char(6) not null comment \u0026#39;资源|id\u0026#39;, primary key (`id`) ) engine=innodb default charset=utf8mb4 comment=\u0026#39;角色资源关联\u0026#39;; insert into `role_resource` values (\u0026#39;00000000\u0026#39;, \u0026#39;00000000\u0026#39;, \u0026#39;01\u0026#39;);   资源表数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  [{ \u0026#34;id\u0026#34;: \u0026#34;00\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;欢迎\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;welcome\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;系统管理\u0026#34;, \u0026#34;children\u0026#34;: [{ \u0026#34;id\u0026#34;: \u0026#34;0101\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;用户管理\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;system/user\u0026#34;, \u0026#34;children\u0026#34;: [ {\u0026#34;id\u0026#34;: \u0026#34;010101\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;保存\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/system/admin/user/list\u0026#34;, \u0026#34;/system/admin/user/save\u0026#34;]}, {\u0026#34;id\u0026#34;: \u0026#34;010102\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;删除\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/system/admin/user/delete\u0026#34;]}, {\u0026#34;id\u0026#34;: \u0026#34;010103\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;重置密码\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/system/admin/user/save-password\u0026#34;]} ] }, { \u0026#34;id\u0026#34;: \u0026#34;0102\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;资源管理\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;system/resource\u0026#34;, \u0026#34;children\u0026#34;: [ {\u0026#34;id\u0026#34;: \u0026#34;010201\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;保存/显示\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/system/admin/resource\u0026#34;]} ] }, { \u0026#34;id\u0026#34;: \u0026#34;0103\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;角色管理\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;system/role\u0026#34;, \u0026#34;children\u0026#34;: [ {\u0026#34;id\u0026#34;: \u0026#34;010301\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;角色/权限管理\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/system/admin/role\u0026#34;]} ] }] }, { \u0026#34;id\u0026#34;: \u0026#34;02\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;业务管理\u0026#34;, \u0026#34;children\u0026#34;: [{ \u0026#34;id\u0026#34;: \u0026#34;0201\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;分类管理\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;business/category\u0026#34;, \u0026#34;children\u0026#34;: [ {\u0026#34;id\u0026#34;: \u0026#34;020101\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;增删改查\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/business/admin/category\u0026#34;]} ] }, { \u0026#34;id\u0026#34;: \u0026#34;0202\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;课程管理\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;business/course\u0026#34;, \u0026#34;children\u0026#34;: [ {\u0026#34;id\u0026#34;: \u0026#34;020201\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;增删改查\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/business/admin/course\u0026#34;, \u0026#34;/business/admin/category/all\u0026#34;]} ] }, { \u0026#34;id\u0026#34;: \u0026#34;0203\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;讲师管理\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;business/teacher\u0026#34;, \u0026#34;children\u0026#34;: [ {\u0026#34;id\u0026#34;: \u0026#34;020301\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;增删改查\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/business/admin/teacher\u0026#34;]} ] }, { \u0026#34;id\u0026#34;: \u0026#34;0204\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;会员管理\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;business/member\u0026#34;, \u0026#34;children\u0026#34;: [ {\u0026#34;id\u0026#34;: \u0026#34;020401\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;增删改查\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/business/admin/member\u0026#34;]} ] }, { \u0026#34;id\u0026#34;: \u0026#34;0205\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;短信管理\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;business/sms\u0026#34;, \u0026#34;children\u0026#34;: [ {\u0026#34;id\u0026#34;: \u0026#34;020501\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;增删改查\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/business/admin/sms\u0026#34;]} ] }] }, { \u0026#34;id\u0026#34;: \u0026#34;03\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;文件管理\u0026#34;, \u0026#34;children\u0026#34;: [{ \u0026#34;id\u0026#34;: \u0026#34;0301\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;文件管理\u0026#34;, \u0026#34;page\u0026#34;: \u0026#34;file/file\u0026#34;, \u0026#34;children\u0026#34;: [ {\u0026#34;id\u0026#34;: \u0026#34;030101\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;文件管理\u0026#34;, \u0026#34;request\u0026#34;: [\u0026#34;/file/admin/file\u0026#34;]} ] }] }]   ","description":"权限管理功能设计和开发","id":11,"section":"posts","tags":[],"title":"权限管理功能设计","uri":"/en/posts/course-online-project/%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%E5%8A%9F%E8%83%BD%E8%AE%BE%E8%AE%A1/"},{"content":" 在调试springboot后端对vue前端发送的包含验证码的request的时候, 由于Session id 不一致, 导致无法获取验证码信息, 在正确的跨域设置下, session id 依旧不同, 之后再打开Firefox浏览器后, 发现一切正常, 发现是chrome浏览器的坑, 新的edge也有同样的安全机制.\n chrome浏览器安全机制 谷歌浏览器的SameSite安全机制的问题，浏览器在跨域的时候不允许request请求携带cookie，导致每次sessionId都是新的, 我的浏览器版本是chrome90.0.4430.212, 新的edge也有同样的安全机制\nchrome浏览器的SameSite设置 直接在地址栏里输入chrome://flags/\n然后在搜索框里搜索关键字SameSite\n找到与之匹配的项SameSite by default cookies\n将其设置为Disabled, 然后关闭浏览器再打开\n如此，同窗口session没超时的情况下，每次请求就不会出现后台sessionId不同的情况\n跨域和保持session一致的设置 Springboot后端路由模块的设置:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  /** * 配置跨域 */ @Bean public CorsWebFilter corsFilter() { CorsConfiguration config = new CorsConfiguration(); config.setAllowCredentials(Boolean.TRUE); config.addAllowedMethod(\u0026#34;*\u0026#34;); config.addAllowedHeader(\u0026#34;*\u0026#34;); config.addAllowedOriginPattern(CorsConfiguration.ALL); config.setMaxAge(3600L); UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(new PathPatternParser()); source.registerCorsConfiguration(\u0026#34;/**\u0026#34;, config); return new CorsWebFilter(source); }   vue前端的main.js设置:\n1 2  // 解决每次ajax请求，对应的sessionId不一致的问题 axios.defaults.withCredentials = true;   ","description":"前后端跨域配置, 以及浏览器问题描述","id":13,"section":"posts","tags":["跨域"],"title":"解决Springboot+vue跨域Request的sessionId不同的问题","uri":"/en/posts/course-online-project/springboot-vue-request-sessionid/"},{"content":"如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客https://johntunliu.gitee.io/或微信公众号LiuJohntun，记录并分享我的所见、所学、所想\u0026hellip;\n","description":"","id":14,"section":"posts","tags":[],"title":"数据分片上传流程图","uri":"/en/posts/%E5%88%86%E7%89%87%E4%B8%8A%E4%BC%A0/"},{"content":" 在idea上操作时没注意到HEAD和master分支已经分离, 导致下意识切回master后代码丢失, git log 无法查看到记录\n 到目录使用git reflog命令 使用命令后找到最后一次修改记录, 复制commit id, 我这是637a530\n然后用命令创建新的分支还原删除的分支\n1  git checkout -b reback_remove_branch 637a530   如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客https://johntunliu.gitee.io/或微信公众号LiuJohntun，记录并分享我的所见、所学、所想\u0026hellip;\n","description":"记录Git仓库错误操作","id":15,"section":"posts","tags":["git"],"title":"Git 从某次提交单独分离HEAD，切换master导致代码丢失的处理","uri":"/en/posts/git-head-master/"},{"content":"不能正常启动vue项目，对比之后发现是没有node_modules文件夹，解决方法就是进入项目文件夹，运行npm安装命令，生成相应文件\n错误日志：\n1 2 3 4 5 6 7  error code ELIFECYCLE error errno 1 error admin@0.1.0 serve-dev: `vue-cli-service serve --mode dev` error Exit status 1 error Failed at the admin@0.1.0 serve-dev script. error This is probably not a problem with npm. There is likely additional logging output above. verbose exit [ 1, true ]   1  输入cnpm install 或 npm install 后，再次启动npm run dev   ","description":"","id":16,"section":"posts","tags":[],"title":"启动Vue提示There is likely additional logging output above","uri":"/en/posts/course-online-project/vue-there-is-likely-additional-logging-output-above/"},{"content":" 报错：Dependency \u0026lsquo;org.dom4j:dom4j:2.1.1\u0026rsquo; not found\n 我开始是直接在最顶层的pom.xml中的\u0026lt;dependencyManagement\u0026gt;标签中导版本包\n然后就一直报Dependency \u0026lsquo;org.dom4j:dom4j:2.1.1\u0026rsquo; not found\n最后解决是直接到子模块中导包，发现可以导入，然后将子模块的版本号去掉\n","description":"报错：Dependency 'org.dom4j:dom4j:2.1.1' not found","id":18,"section":"posts","tags":[],"title":"关于maven无法导包解决Dependency not found","uri":"/en/posts/course-online-project/%E5%85%B3%E4%BA%8Emaven%E6%97%A0%E6%B3%95%E5%AF%BC%E5%8C%85%E8%A7%A3%E5%86%B3dependency-not-found/"},{"content":"稳定vue分页组件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154  \u0026lt;!--pagination.vue--\u0026gt; \u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;pagination\u0026#34; role=\u0026#34;group\u0026#34; aria-label=\u0026#34;分页\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default btn-white btn-round\u0026#34; v-bind:disabled=\u0026#34;page === 1\u0026#34; v-on:click=\u0026#34;selectPage(1)\u0026#34;\u0026gt; 1 \u0026lt;/button\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default btn-white btn-round\u0026#34; v-bind:disabled=\u0026#34;page === 1\u0026#34; v-on:click=\u0026#34;selectPage(page - 1)\u0026#34;\u0026gt; 上一页 \u0026lt;/button\u0026gt; \u0026lt;button v-for=\u0026#34;p in pages\u0026#34; v-bind:id=\u0026#34;\u0026#39;page-\u0026#39; + p\u0026#34; type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default btn-white btn-round\u0026#34; v-bind:class=\u0026#34;{\u0026#39;btn-primary active\u0026#39;:page == p}\u0026#34; v-on:click=\u0026#34;selectPage(p)\u0026#34;\u0026gt; {{p}} \u0026lt;/button\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default btn-white btn-round\u0026#34; v-bind:disabled=\u0026#34;page === pageTotal\u0026#34; v-on:click=\u0026#34;selectPage(page + 1)\u0026#34;\u0026gt; 下一页 \u0026lt;/button\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default btn-white btn-round\u0026#34; v-bind:disabled=\u0026#34;page === pageTotal\u0026#34; v-on:click=\u0026#34;selectPage(pageTotal)\u0026#34;\u0026gt; {{pageTotal||1}} \u0026lt;/button\u0026gt; \u0026amp;nbsp; \u0026lt;span class=\u0026#34;m--padding-10\u0026#34;\u0026gt; 每页 \u0026lt;select v-model=\u0026#34;size\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;1\u0026#34;\u0026gt;1\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;5\u0026#34;\u0026gt;5\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;10\u0026#34;\u0026gt;10\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;20\u0026#34;\u0026gt;20\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;50\u0026#34;\u0026gt;50\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;100\u0026#34;\u0026gt;100\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; 条，共【{{total}}】条 \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; export default { name: \u0026#39;pagination\u0026#39;, props: { list: { type: Function, default: null }, itemCount: Number // 显示的页码数，比如总共有100页，只显示10页，其它用省略号表示  }, data: function () { return { total: 0, // 总行数  size: 10, // 每页条数  page: 0, // 当前页码  pageTotal: 0, // 总页数  pages: [], // 显示的页码数组  } }, methods: { /** * 渲染分页组件 * @param page * @param total */ render(page, total) { let _this = this; _this.page = page; _this.total = total; _this.pageTotal = Math.ceil(total / _this.size); _this.pages = _this.getPageItems(_this.pageTotal, page, _this.itemCount || 5); }, /** * 查询某一页 * @param page */ selectPage(page) { let _this = this; if (page \u0026lt; 1) { page = 1; } if (page \u0026gt; _this.pageTotal) { page = _this.pageTotal; } if (this.page !== page) { _this.page = page; if (_this.list) { _this.list(page); } } }, /** * 当前要显示在页面上的页码 * @param total * @param current * @param length * @returns {Array} */ getPageItems(total, current, length) { let items = []; if (length \u0026gt;= total) { for (let i = 1; i \u0026lt;= total; i++) { items.push(i); } } else { let base = 0; // 前移  if (current - 0 \u0026gt; Math.floor((length - 1) / 2)) { // 后移  base = Math.min(total, current - 0 + Math.ceil((length - 1) / 2)) - length; } for (let i = 1; i \u0026lt;= length; i++) { items.push(base + i); } } return items; } } } \u0026lt;/script\u0026gt; \u0026lt;style scoped\u0026gt; .pagination { vertical-align: middle !important; font-size: 16px; margin-top: 0; margin-bottom: 10px; } .pagination button { margin-right: 5px; } .btn-primary.active { background-color: #2f7bba !important; border-color: #27689d !important; color: white !important; font-weight: 600; } /*.pagination select {*/ /*vertical-align: middle !important;*/ /*font-size: 16px;*/ /*margin-top: 0;*/ /*}*/ \u0026lt;/style\u0026gt;   ","description":"稳定vue分页组件","id":19,"section":"posts","tags":["vue"],"title":"vue通用pagination分页组件","uri":"/en/posts/course-online-project/pagination/"},{"content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 修改一下路径--\u0026gt; \u0026lt;property name=\u0026#34;PATH\u0026#34; value=\u0026#34;/log/imooc/course/eureka\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder\u0026gt; \u0026lt;!-- \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} %highlight(%-5level) %blue(%-50logger{50}:%-4line) %msg%n\u0026lt;/Pattern\u0026gt;--\u0026gt; \u0026lt;Pattern\u0026gt;%d{ss.SSS} %highlight(%-5level) %blue(%-30logger{30}:%-4line) %msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;TRACE_FILE\u0026#34; class=\u0026#34;ch.qos.logback.core.rolling.RollingFileAppender\u0026#34;\u0026gt; \u0026lt;file\u0026gt;${PATH}/trace.log\u0026lt;/file\u0026gt; \u0026lt;rollingPolicy class=\u0026#34;ch.qos.logback.core.rolling.TimeBasedRollingPolicy\u0026#34;\u0026gt; \u0026lt;FileNamePattern\u0026gt;${PATH}/trace.%d{yyyy-MM-dd}.%i.log\u0026lt;/FileNamePattern\u0026gt; \u0026lt;timeBasedFileNamingAndTriggeringPolicy class=\u0026#34;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\u0026#34;\u0026gt; \u0026lt;maxFileSize\u0026gt;10MB\u0026lt;/maxFileSize\u0026gt; \u0026lt;/timeBasedFileNamingAndTriggeringPolicy\u0026gt; \u0026lt;/rollingPolicy\u0026gt; \u0026lt;layout\u0026gt; \u0026lt;pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %-50logger{50}:%-4line %green(%-8X{UUID}) %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;ERROR_FILE\u0026#34; class=\u0026#34;ch.qos.logback.core.rolling.RollingFileAppender\u0026#34;\u0026gt; \u0026lt;file\u0026gt;${PATH}/error.log\u0026lt;/file\u0026gt; \u0026lt;rollingPolicy class=\u0026#34;ch.qos.logback.core.rolling.TimeBasedRollingPolicy\u0026#34;\u0026gt; \u0026lt;FileNamePattern\u0026gt;${PATH}/error.%d{yyyy-MM-dd}.%i.log\u0026lt;/FileNamePattern\u0026gt; \u0026lt;timeBasedFileNamingAndTriggeringPolicy class=\u0026#34;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\u0026#34;\u0026gt; \u0026lt;maxFileSize\u0026gt;10MB\u0026lt;/maxFileSize\u0026gt; \u0026lt;/timeBasedFileNamingAndTriggeringPolicy\u0026gt; \u0026lt;/rollingPolicy\u0026gt; \u0026lt;layout\u0026gt; \u0026lt;pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %-50logger{50}:%-4line %green(%-8X{UUID}) %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;filter class=\u0026#34;ch.qos.logback.classic.filter.LevelFilter\u0026#34;\u0026gt; \u0026lt;level\u0026gt;ERROR\u0026lt;/level\u0026gt; \u0026lt;onMatch\u0026gt;ACCEPT\u0026lt;/onMatch\u0026gt; \u0026lt;onMismatch\u0026gt;DENY\u0026lt;/onMismatch\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;ERROR\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;ERROR_FILE\u0026#34; /\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;root level=\u0026#34;TRACE\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;TRACE_FILE\u0026#34; /\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34; /\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt;   ","description":"个人使用的日志配置文件","id":20,"section":"posts","tags":["springboot"],"title":"springboot的logback.xml 日志配置","uri":"/en/posts/course-online-project/springboot-logback.xml/"},{"content":"https://github.com/JohntunLiu/course-online\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  @SpringBootApplication @EnableEurekaServer @Slf4j public class EurekaApplication { //\tprivate static final Logger LOG = LoggerFactory.getLogger(EurekaApplication.class);  private static String port2; //获取port值 \t@Value(\u0026#34;${server.port}\u0026#34;) public String port; @PostConstruct public void getPort() { port2=this.port; } public static void main(String[] args) { SpringApplication.run(EurekaApplication.class, args); log.info(\u0026#34;启动成功！！\u0026#34;); log.info(\u0026#34;Eureka地址: \\thttp://127.0.0.1:\u0026#34;+port2); }   ","description":"spring获取yml配置中的键值方法","id":21,"section":"posts","tags":["springboot"],"title":"springboot使用@value 获取yml配置中的值","uri":"/en/posts/course-online-project/springboot-value-yml/"},{"content":" 我现在发文的操作是：在Forestry.io上管理和编辑文章预览无误后保存，之后就是自动流程了，保存后会push到GitHub上的myblog仓库，push操作会触发GitHub Actions脚本，然后脚本运行生成静态网站数据，再同步到GitHub和Gitee的pages所在仓库，会顺带把Gitee Pages页面也刷新一下，所以博客的内容发布和管理全在一个[CMS]( \u0026ldquo;内容管理系统(Content Management System)\u0026quot;)平台上管理。具体操作容在我博客里已经写了，欢迎访问，下文记录我搭建总体过程。\n Hugo静态网站生成器 用go语言编写号称最快的开源静态网站生成器，被我选中用来充当建站的开端，可以直接看官方文档，有不懂的单词的话，我是直接用的桌面般的欧路词典，哪里不会就点哪里，说明文档一般都比较容易懂，毕竟是写说明文嘛，不会去堆叠华丽的词藻。\n需要注意的地方：  可以选择主题说明详细的上手，连命令代码都准备好了 刚开始多试试几个主题，有些非常容易使用，感谢作者 注意有些主题要用hugo_extended的软件版本来生成，这上边卡了许久  GitHub Actions事件驱动工作流 网站能自动部署全都依仗于该功能，大多是用别人的开源代码，我们只需配置代码运行所需参数，调整执行流程，感谢开源作者，也有中文文档\n需要注意的地方：  注意参数名和参数对应上 直接看GitHub上开源仓库的说明，网上搜到的会不详细或者没有更新出现的问题 根据代码的使用搜到GitHub上的开源库  GitHub Pages 这个不多说，创建设置里面直接开启就行了，也会自动发布，giteepages的自动发布好像需要会员，但在正常访问困难。\nGitee Pages 在服务里打开设置\n需要注意的地方：  在设置deploy key 时，注意创建可写入的  Forestry.io内容管理系统 一款基于管理类似于GitHub这类仓库md文件的管理系统forestry，内容编辑，管理和发布全在这上面完成，而且极易上手且免费，文章地址\n如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客https://johntunliu.gitee.io/或微信公众号LiuJohntun，记录并分享我的所见、所学、所想\u0026hellip;\n","description":"简述一套搭建博客的技术，记录搭建过程和踩的坑","id":22,"section":"posts","tags":["博客搭建"],"title":"博客搭建: Hugo GitHubActions GitHubPages GiteePages Forestry.io","uri":"/en/posts/hugo-githubactions-githubpages-giteepages-forestry.io/"},{"content":" 本文主要描述个人如何使用GitHub Actions生成静态网站自动发布到阿里OSS上并绑定个人域名访问的，分享一下踩坑经验，希望能对你有所帮助。\n OSS中创建Bucket 这里注意如果域名没备案的话，可以选择境外地域的结点，下图以新加坡为例：\n修改OSS写读权限 获取AccessKey 使用的是阿里推荐的子用户**AccessKey，**可以配置权限\nGitHub填入Accesskey ID和Secret GitHub Actions编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  name:deploy blogon:push:schedule:- cron:\u0026#39;30 20 * * *\u0026#39;jobs:build:runs-on:ubuntu-lateststeps:- uses:actions/checkout@v2with:submodules:\u0026#39;recursive\u0026#39;- uses:srt32/git-actions@v0.0.3with:args:git submodule update --init --recursive- name:use Node.js 10.xuses:actions/setup-node@v1with:node-version:10.x- name:npm i Hugouses:peaceiris/actions-hugo@v2- name:Buildrun:|hugo --minify #hugo --minify到ubuntu-latest是部署Hugo的，不用可以去掉- name:setup aliyun ossuses:manyuanrong/setup-ossutil@masterwith:endpoint:oss-ap-southeast-1.aliyuncs.com#修改为自己的Endpointaccess-key-id:${{ secrets.OSS_KEY_ID }}access-key-secret:${{ secrets.OSS_KEY_SECRET }}- name:cp aliyun ossrun:ossutil cp -rf public oss://zongtang/#修改为自己的  注意代码中的注释内容\n运行效果 解析域名 我是用的腾讯提供的域名服务，因为记录值不是IP地址，所以记录类型就选CNAME，记录值就填OSS概述中的域名，第一张图里标示了\n测试访问 欢迎去我博客或微信公众号LiuJohntun看整个建站流程，目前已经简化操作直接在Forestry.io上编写markdown文章后预览无误后提交通过GitHub Actions自动部署到GitHubPages、 GiteePages、OSS对象存储\n如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客或微信公众号LiuJohntun，记录并分享我的所见、所学、所想\u0026hellip;\n","description":"","id":24,"section":"posts","tags":["博客优化"],"title":"Github Actions自动部署Hugo到阿里OSS","uri":"/en/posts/github-actions-hugo-oss/"},{"content":" 相信大多博客都是生成静态网站搭建的，当然就需要管理大量的markdown文件，我偶然用到了一款基于管理类似于GitHub这类仓库md文件的管理系统forestry，极易上手，可以在线使用网站的生成器进行静态网站的生成，在线直接预览，我是用它和GitHub Actions搭配，只需在这里提交就自动部署到Gitee pages或是GitHub pages。接下来就是快速上手教程。https://forestry.io/\n 支持的静态网站生成器 支持的git provider 选择GitHub后就会进入登录界面，如果要加载私有库需要点击上面这个选项，Check For Config 就是指定Config.toml 在哪里\n主要设置步骤 可以看到第一部的仓库配置已经自动完成了配置\n它会去指定仓库设置一个Deploy keys用于部署仓库\n其实弄完第二步就可以使用了，配置的主要步骤：\n  配置仓库，在创建站点时就完成了\n  是指定文件位置扫描md文件，会在侧边栏显示，可以分类显示，我只用了Directory这种类型\n  是指定媒体储存位置，我用不上就没设置\n  就是配置静态网站生成器的，在创建的时候就配置了，在这可以调整\n  具体使用 创建博文模板Front Matter 创建一个md模板，基于现有文件来创建，Front Matter选项\n里面就是md文件头部的参数设置，根据实际调整\n创建博文Create new 创建时选择模板，只有一个模板时不能选择，会直接使用模板\n点击保存就会同步到仓库了\n进行预览，几秒就可生成了\n欢迎去我博客或微信公众号LiuJohntun看整个建站流程，目前已经简化操作直接在Forestry.io上编写markdown文章后预览无误后提交通过GitHub Actions自动部署到GitHubPages、 GiteePages、OSS对象存储\n如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客https://johntunliu.github.io/或微信公众号LiuJohntun，记录并分享我的所见、所学、所想…\n","description":"","id":25,"section":"posts","tags":["内容管理","博客优化"],"title":"超好用静态网站的markdown内容管理系统forestry快速上手","uri":"/en/posts/markdown/"},{"content":" 我的博客使用GitHub上的pages功能发布的基于Hugo生成的静态网站，基本无法正常访问，所以想要同步一份到gitee上发布，现在使用GitHub Actions提供的计算机资源就可以直接在GitHub上进行静态网站的生成，发布，远程刷新gitee pages，触发条件可以是push或者定时等等，可谓十分好用，之后看到可以直接同步到gitee仓库，就实现一下试试，以下就是实现步骤，以及踩坑，当然强烈建议看开源代码的官方说明文档。\n 生成公钥和私钥并填入仓库 输入ssh-keygen -t rsa -C \u0026quot;user@email.com\u0026quot;，然后回车几次，会生成 id_rsa.pub 文件和 id_rsa 文件，分别存放公钥和私钥：\nGitee仓库填入公钥 将公钥 id_rsa.pub 中的数据填入到gitee待备份仓库界面下 settings→Deploy keys→add personal public key\n**这里注意:**要选右上添加personal public key才有写入权限\nGitHub仓库填入私钥 Settings→Secret→New repository secre 用于之后的程序环境配置访问，命名为GITEE_RSA_PRIVATE_KEY\n生成GitHub账号的 personal access token 将仓库权限选上就行了，然后将生成的token，配到私钥配置的地方 仓库→Settings→Secret→New repository secre，命名为ACCESS_TOKEN\n在仓库secret处添加GITEE_PASSWORD，放入gitee账号密码用于刷新gitee pages 同之前步骤相同，之后用于环境变量的配置，就是以下3条secret，OSS的是自动部署到阿里OSS的脚本使用的，我是放在一个脚本里运行，需要了解可以看我另一篇文章。\n在GitHub仓库创建并编写Actions脚本！！！ 文件名随意从这点开就行，下面有模板，点开后修改也行，创建的文件默认放在.github/workflows/下\n也可以用命令创建mkdir -p .github/workflows \u0026amp;\u0026amp; touch .github/workflows/name.yml\n将代码拷入，修改具体变量，比如仓库名等，如果不需要deploy直接去掉就行了，不影响： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  name:deploy blog to giteeon:push:branches:- main # master 分支 push 的时候触发jobs:deploy:#执行部署Hugo生成静态代码，默认放在gh-pages分支runs-on:ubuntu-18.04steps:- uses:actions/checkout@v2with:submodules:recursive # Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:\u0026#39;0.81.0\u0026#39;extended:true#不需要extended版本就可以注释- name:Buildrun:hugo --minify- name:Deploypageuses:peaceiris/actions-gh-pages@v3with:personal_token:${{ secrets.ACCESS_TOKEN }}external_repository:JohntunLiu/JohntunLiu.github.iopublish_branch: gh-pages # default:gh-pagespublish_dir:./public- name:Deploygiteeuses:peaceiris/actions-gh-pages@v3with:personal_token:${{ secrets.ACCESS_TOKEN }}publish_dir:./publicsync:#同步到gitee仓库needs:deployruns-on:ubuntu-lateststeps:- name:Sync to Giteeuses:wearerequired/git-mirror-action@masterenv:SSH_PRIVATE_KEY:${{ secrets.GITEE_RSA_PRIVATE_KEY }}with:# 来源仓库source-repo:\u0026#34;git@github.com:JohntunLiu/myblog.git\u0026#34;# 目标仓库destination-repo:\u0026#34;git@gitee.com:JohntunLiu/JohntunLiu.git\u0026#34;reload-pages:needs:syncruns-on:ubuntu-lateststeps:- name:Build Gitee Pagesuses:yanglbme/gitee-pages-action@mainwith:# 注意替换为你的 Gitee 用户名gitee-username:JohntunLiu# 注意在 Settings-\u0026gt;Secrets 配置 GITEE_PASSWORDgitee-password:${{ secrets.GITEE_PASSWORD }}# 注意替换为你的 Gitee 仓库，仓库名严格区分大小写，请准确填写，否则会出错gitee-repo:JohntunLiu/JohntunLiu# 要部署的分支，默认是 master，若是其他分支，则需要指定（指定的分支必须存在）branch:gh-pages  点击commit changes 提交运行，之后就看得到运行流程了\n实际效果和流程 如果是部署其他静态网站，修改deploy的代码块就行了，我顺便把部署到GitHub pages放在了里面：name: Deploypage\n如果是hugo的话开源人员还提供了缓存机制，可以提高部署速度，可以去开源部分看，具体怎么看就是复制- uses: peaceiris/actions-hugo@v2 后面的部分搜索到GitHub中看，比如：https://github.com/peaceiris/actions-hugo，readme.md文档写得相当详实，也会更新说明\n欢迎去我博客或微信公众号LiuJohntun看整个建站流程，目前已经简化操作直接在Forestry.io上编写markdown文章后预览无误后提交通过GitHub Actions自动部署到GitHubPages、 GiteePages、OSS对象存储\n如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客或微信公众号LiuJohntun，记录并分享我的所见、所学、所想\u0026hellip;\n","description":"","id":26,"section":"posts","tags":["博客优化"],"title":"Github Actions自动部署Hugo到Gitee同时刷新Gitee Pages","uri":"/en/posts/%E9%80%9A%E8%BF%87githubactions%E5%B0%86%E4%BB%93%E5%BA%93%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD%E5%88%B0gitee/"},{"content":"从设置首页打开语言栏，也可以直接搜索，再点击语言中的 选项 点击微软拼音的 选项 选择 词库和自学 选中 添加和编辑自定义短语 添加 编辑短语，可以调整在输入法中的位置 使用输入法时就可以使用该快捷短语了 完成常用代码的快速输入 如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客或微信公众号LiuJohntun，记录并分享我的所见、所学、所想\u0026hellip;\n","description":"","id":27,"section":"posts","tags":["实用技巧"],"title":"Win10输入法自定义短语设置 提升个人效率","uri":"/en/posts/win10%E8%BE%93%E5%85%A5%E6%B3%95%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9F%AD%E8%AF%AD%E8%AE%BE%E7%BD%AE-%E6%8F%90%E5%8D%87%E4%B8%AA%E4%BA%BA%E6%95%88%E7%8E%87/"},{"content":" When allowCredentials is true, allowedOrigins cannot contain the special value \u0026ldquo;*“since that cannot be set on the “Access-Control-Allow-Origin” response header. To allow credentials to a set of origins, list them explicitly or consider using\u0026quot;allowedOriginPatterns” instead.\n  当allowCredentials为true时，allowedOrigins不能包含特殊值“ *”，因为无法在“ Access-Control-Allow-Origin”响应标头上设置。要允许凭据为一组来源，请明确列出它们或考虑使用“ allowedOriginPatterns” 代替。\n 将跨域设置放在gateway模块中:\n解决办法：跨域配置报错，将.allowedOrigins替换成.allowedOriginPatterns即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  /** * 配置跨域 * @return */ @Bean public CorsWebFilter corsFilter() { CorsConfiguration config = new CorsConfiguration(); config.setAllowCredentials(Boolean.TRUE); config.addAllowedMethod(\u0026#34;*\u0026#34;); config.addAllowedOriginPattern(\u0026#34;*\u0026#34;); config.addAllowedHeader(\u0026#34;*\u0026#34;); config.setMaxAge(3600L); UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(new PathPatternParser()); source.registerCorsConfiguration(\u0026#34;/**\u0026#34;, config); return new CorsWebFilter(source); }   ","description":"","id":28,"section":"posts","tags":["Springboot"],"title":"Springboot跨域配置报特殊符号错误","uri":"/en/posts/springboot-%E8%B7%A8%E5%9F%9F%E9%85%8D%E7%BD%AE%E6%8A%A5%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%E9%94%99%E8%AF%AF/"},{"content":"目的是固定虚拟机CentOS7 中IP地址操作，以免重启后IP地址改变 网络信息截图如下，读取ip地址和网关等信息：\n自动分配网络内可用IP：\n1  [root@tang ~]# dhclient   固定静态IP：\n1  [root@tang ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33    文件初始：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  TYPE=\u0026#34;Ethernet\u0026#34;PROXY_METHOD=\u0026#34;none\u0026#34;BROWSER_ONLY=\u0026#34;no\u0026#34;BOOTPROTO=\u0026#34;dhcp\u0026#34;DEFROUTE=\u0026#34;yes\u0026#34;IPV4_FAILURE_FATAL=\u0026#34;no\u0026#34;IPV6INIT=\u0026#34;yes\u0026#34;IPV6_AUTOCONF=\u0026#34;yes\u0026#34;IPV6_DEFROUTE=\u0026#34;yes\u0026#34;IPV6_FAILURE_FATAL=\u0026#34;no\u0026#34;IPV6_ADDR_GEN_MODE=\u0026#34;stable-privacy\u0026#34;NAME=\u0026#34;ens33\u0026#34;UUID=\u0026#34;d5c29673-866e-4fa4-8613-d2c11cccc16c\u0026#34;DEVICE=\u0026#34;ens33\u0026#34;ONBOOT=\u0026#34;yes\u0026#34;  文件修改后：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  TYPE=\u0026#34;Ethernet\u0026#34;PROXY_METHOD=\u0026#34;none\u0026#34;BROWSER_ONLY=\u0026#34;no\u0026#34;# 该处修改为staticBOOTPROTO=\u0026#34;static\u0026#34;DEFROUTE=\u0026#34;yes\u0026#34;IPV4_FAILURE_FATAL=\u0026#34;no\u0026#34;IPV6INIT=\u0026#34;yes\u0026#34;IPV6_AUTOCONF=\u0026#34;yes\u0026#34;IPV6_DEFROUTE=\u0026#34;yes\u0026#34;IPV6_FAILURE_FATAL=\u0026#34;no\u0026#34;IPV6_ADDR_GEN_MODE=\u0026#34;stable-privacy\u0026#34;NAME=\u0026#34;ens33\u0026#34;UUID=\u0026#34;d5c29673-866e-4fa4-8613-d2c11cccc16c\u0026#34;DEVICE=\u0026#34;ens33\u0026#34;ONBOOT=\u0026#34;yes\u0026#34;# 该处添加网络信息IPADDR=\u0026#34;192.168.12.131\u0026#34;NETMASK=\u0026#34;255.255.255.0\u0026#34;GATEWAY=\u0026#34;192.168.12.1\u0026#34;DNS1=\u0026#34;119.29.29.29\u0026#34;  重启网络：\n1  systemctl restart network.service   如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客或微信公众号LiuJohntun，记录并分享我的所见、所学、所想\u0026hellip;\n","description":"","id":29,"section":"posts","tags":["CentOS"],"title":"固定虚拟机CentOS7中IP地址的操作","uri":"/en/posts/%E5%9B%BA%E5%AE%9Acentos7%E4%B8%ADip%E5%9C%B0%E5%9D%80%E6%93%8D%E4%BD%9C/"},{"content":"Top Ten Eleven 1504 Tips  Oh, the questions you’ll ask 注意你将要问的问题   Questions will very often determine the quest that you take\n问题往往会决定你所追求的事物\n  We also need to ask the positive question\n我们要问那种积极的问题\n  Questions start quest\n问题会是你探索事物的起点\n  Believe—in yourself and others 相信自己和他人   Beliefs create reality\n信念创造现实\n  Two things distinguish the extraordinary from the rest:\n非凡的人常做的两件事：\n  One was that they were always asking question, always wanting to learn more, having that humbleness\n一是他们总是在问问题，总是想学更多，同时又十分谦虚\n  They believed in themselves\n他们相信他们自己\n    Learn to fail or fail to learn 学会失败 从失败中学习 Give yourself the permission to be human 承认自己是个凡人 Open up (journal and/or person) 打开心扉（记日记或者向人倾诉） Cultivate the benefit finder (express gratitude) 培养自己成为美好事物的发现者（心怀感激） Simplify 简化   But remember that a much better predictor of well-being than material affluence is time affluence.\n但是请记住，比起物质富裕，时间富裕能更好地预测幸福。\n  Say NO at times when it\u0026rsquo;s appropriate.\n在适当的时候说不\n  Find out what you really , really want to do with your life and do it\n找出您真正想要做的事，并去做\n  Cultivate relationships 培养良好的人际关系 Remember the mind-body connection 记住身体和心灵的联系   Exercise at least four times a week for 30 minutes\n每周至少四次运动30分钟\n  Yoga or mindful meditation, breathe deeply\n瑜伽，冥想，深呼吸\n  Sleep around 8 hours a night of sleep\n每晚约睡8个小时\n  Touch, hug and embrace\n  多与他人接触和拥抱\nStrive to be known rather than validated 努力让他人了解自己而不是被认可 Introduce behavioral change NOW 以改变行为当作起点，马上行动 如若内容有不足之处，还望大家多提建议多多与我交流。\n更多精彩可以关注我的博客或微信公众号LiuJohntun，记录并分享我的所见、所学、所想\u0026hellip;\n","description":"","id":32,"section":"posts","tags":["课程"],"title":"Top Eleven 1504 Tips","uri":"/en/posts/top-eleven-1504-tips/"},{"content":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nSimple button\nbutton   You can specify width, height\nbutton   Set color\nYou can customize the button primary color in the file at root/data/button.toml. Just copy-paste the theme’s button.toml file and edit the params as you want.\nbutton  \nHow to use tab\nWindows MacOS Ubuntu  Windows section 1  console.log(\u0026#39;Hello World!\u0026#39;);     MacOS section Hello world!  Ubuntu section Great!    'use strict'; var containerId = JSON.parse(\"\\\"12abf4928132c6bd\\\"\"); var containerElem = document.getElementById(containerId); var tabLinks = null; var tabContents = null; var ids = []; if (containerElem) { tabLinks = containerElem.querySelectorAll('.tab__link'); tabContents = containerElem.querySelectorAll('.tab__content'); } for (var i = 0; i 0) { tabContents[0].style.display = 'block'; } \nHow to use notice : success, info, warning, error\nsuccess text  info text  warning text  error text  How to use image\nSample Image: Image with title, caption, alt, ...  \nHow to use expand\n  Expand me  Some Markdown Contents taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SC taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SC\ntaxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SC\ntaxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SC\n \nHow to use code java javascript  1  System.out.println(\u0026#39;Hello World!\u0026#39;);     1  console.log(\u0026#39;Hello World!\u0026#39;);       'use strict'; var containerId = JSON.parse(\"\\\"7d54f0bf9d841150\\\"\"); var containerElem = document.getElementById(containerId); var codetabLinks = null; var codetabContents = null; var ids = []; if (containerElem) { codetabLinks = containerElem.querySelectorAll('.codetab__link'); codetabContents = containerElem.querySelectorAll('.codetab__content'); } for (var i = 0; i 0) { codetabContents[0].style.display = 'block'; }  warning, success, info, danger this is a text this is a text this is a text this is a text Markdownify box\nSome markdown contents Simple box\nSome contents  Hugo makes use of a variety of open source projects including:\n https://github.com/russross/blackfriday https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper  Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub.\n","description":"Hugo, the world’s fastest framework for building websites","id":36,"section":"","tags":null,"title":"About","uri":"/en/about/"}]